{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), use_bias=False, padding=\"valid\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:43: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((3, 3), padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), use_bias=False, padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:46: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:465: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), use_bias=False, padding=\"valid\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 7), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (7, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:56: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), use_bias=False, padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:59: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((3, 3), padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:61: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (1, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (3, 3), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:80: UserWarning: Update your `AveragePooling2D` call to the Keras 2 API: `AveragePooling2D((3, 3), padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:83: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(384, (3, 3), use_bias=False, padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (1, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(224, (3, 3), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), use_bias=False, padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:151: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((3, 3), padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:153: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(384, (1, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(224, (1, 7), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (7, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (7, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(224, (7, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 7), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:105: UserWarning: Update your `AveragePooling2D` call to the Keras 2 API: `AveragePooling2D((3, 3), padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:108: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (7, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(320, (3, 3), use_bias=False, padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:171: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D((3, 3), padding=\"valid\", strides=(2, 2))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:173: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (1, 3), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:123: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(448, (3, 1), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (1, 3), use_bias=False, padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:130: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:132: UserWarning: Update your `AveragePooling2D` call to the Keras 2 API: `AveragePooling2D((3, 3), padding=\"same\", strides=(1, 1))`\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:135: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:219: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=1001)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/titu1994/Inception-v4/releases/download/v1.2/inception_v4_weights_tf_dim_ordering_th_kernels.h5\n",
      "171802624/171799220 [==============================] - 9s 0us/step\n",
      "Model weights loaded.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 149, 149, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 149, 149, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 149, 149, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 147, 147, 32) 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 147, 147, 32) 128         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 147, 147, 32) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 147, 147, 64) 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 147, 147, 64) 256         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 147, 147, 64) 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 73, 73, 96)   55296       activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 73, 73, 96)   384         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 73, 73, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 73, 73, 96)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "merge_1 (Merge)                 (None, 73, 73, 160)  0           max_pooling2d_1[0][0]            \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 73, 73, 64)   10240       merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 73, 73, 64)   256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 73, 73, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 73, 73, 64)   28672       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 73, 73, 64)   256         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 73, 73, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 73, 73, 64)   10240       merge_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 73, 73, 64)   28672       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 73, 73, 64)   256         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 73, 73, 64)   256         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 73, 73, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 73, 73, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 71, 71, 96)   55296       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 71, 71, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 71, 71, 96)   384         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 71, 71, 96)   384         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 71, 71, 96)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 71, 71, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_2 (Merge)                 (None, 71, 71, 192)  0           activation_6[0][0]               \n",
      "                                                                 activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 35, 35, 192)  331776      merge_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 35, 35, 192)  768         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 35, 35, 192)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 35, 35, 192)  0           merge_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_3 (Merge)                 (None, 35, 35, 384)  0           activation_11[0][0]              \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 35, 35, 64)   24576       merge_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 35, 35, 64)   256         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 35, 35, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 35, 35, 64)   24576       merge_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 35, 35, 96)   55296       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 35, 35, 64)   256         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 35, 35, 96)   384         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 35, 35, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 35, 35, 96)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 35, 35, 384)  0           merge_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 35, 35, 96)   36864       merge_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 35, 35, 96)   55296       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 35, 35, 96)   82944       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 35, 35, 96)   36864       average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 35, 35, 96)   384         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 35, 35, 96)   384         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 35, 35, 96)   384         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 35, 35, 96)   384         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 35, 35, 96)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 35, 35, 96)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 35, 35, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 35, 35, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_4 (Merge)                 (None, 35, 35, 384)  0           activation_12[0][0]              \n",
      "                                                                 activation_14[0][0]              \n",
      "                                                                 activation_17[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 35, 35, 64)   24576       merge_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 35, 35, 64)   256         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 35, 35, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 35, 35, 64)   24576       merge_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 35, 35, 96)   55296       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 35, 35, 64)   256         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 35, 35, 96)   384         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 35, 35, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 35, 35, 96)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 35, 35, 384)  0           merge_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 35, 35, 96)   36864       merge_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 35, 35, 96)   55296       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 35, 35, 96)   82944       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 35, 35, 96)   36864       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 35, 35, 96)   384         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 35, 35, 96)   384         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 35, 35, 96)   384         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 35, 35, 96)   384         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 35, 35, 96)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 35, 35, 96)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 35, 35, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 35, 35, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_5 (Merge)                 (None, 35, 35, 384)  0           activation_19[0][0]              \n",
      "                                                                 activation_21[0][0]              \n",
      "                                                                 activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 35, 35, 64)   24576       merge_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 35, 35, 64)   256         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 35, 35, 64)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 35, 35, 64)   24576       merge_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 35, 35, 96)   55296       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 35, 35, 64)   256         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 35, 35, 96)   384         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 35, 35, 64)   0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 35, 35, 96)   0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 35, 35, 384)  0           merge_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 35, 35, 96)   36864       merge_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 35, 35, 96)   55296       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 35, 35, 96)   82944       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 35, 35, 96)   36864       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 35, 35, 96)   384         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 35, 35, 96)   384         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 35, 35, 96)   384         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 35, 35, 96)   384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 35, 35, 96)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 35, 35, 96)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 35, 35, 96)   0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 35, 35, 96)   0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_6 (Merge)                 (None, 35, 35, 384)  0           activation_26[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "                                                                 activation_31[0][0]              \n",
      "                                                                 activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 35, 35, 64)   24576       merge_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 35, 35, 64)   256         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 35, 35, 64)   0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 35, 35, 64)   24576       merge_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 35, 35, 96)   55296       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 35, 35, 64)   256         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 35, 35, 96)   384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 35, 35, 64)   0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 35, 35, 96)   0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 35, 35, 384)  0           merge_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 35, 35, 96)   36864       merge_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 35, 35, 96)   55296       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 35, 35, 96)   82944       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 35, 35, 96)   36864       average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 35, 35, 96)   384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 35, 35, 96)   384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 35, 35, 96)   384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 35, 35, 96)   384         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 35, 35, 96)   0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 35, 35, 96)   0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 35, 35, 96)   0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 35, 35, 96)   0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_7 (Merge)                 (None, 35, 35, 384)  0           activation_33[0][0]              \n",
      "                                                                 activation_35[0][0]              \n",
      "                                                                 activation_38[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 35, 35, 192)  73728       merge_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 35, 35, 192)  768         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 35, 35, 192)  0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 35, 35, 224)  387072      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 35, 35, 224)  896         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 35, 35, 224)  0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 17, 17, 384)  1327104     merge_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 17, 17, 256)  516096      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 17, 17, 384)  1536        conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 17, 17, 256)  1024        conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 17, 17, 384)  0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 17, 17, 256)  0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 17, 17, 384)  0           merge_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "merge_8 (Merge)                 (None, 17, 17, 1024) 0           activation_40[0][0]              \n",
      "                                                                 activation_43[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 17, 17, 192)  196608      merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 17, 17, 192)  768         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 17, 17, 192)  0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 17, 17, 192)  258048      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 17, 17, 192)  768         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 17, 17, 192)  0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 17, 17, 192)  196608      merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 17, 17, 224)  301056      activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 17, 17, 192)  768         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 17, 17, 224)  896         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 17, 17, 192)  0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 17, 17, 224)  0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 17, 17, 224)  301056      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 17, 17, 224)  351232      activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 17, 17, 224)  896         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 17, 17, 224)  896         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 17, 17, 224)  0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 17, 17, 224)  0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 17, 17, 1024) 0           merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 17, 17, 384)  393216      merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 17, 17, 256)  401408      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 17, 17, 256)  401408      activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 17, 17, 128)  131072      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 17, 17, 384)  1536        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 17, 17, 256)  1024        conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 17, 17, 256)  1024        conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 17, 17, 128)  512         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 17, 17, 384)  0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 17, 17, 256)  0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 17, 17, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 17, 17, 128)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_9 (Merge)                 (None, 17, 17, 1024) 0           activation_44[0][0]              \n",
      "                                                                 activation_47[0][0]              \n",
      "                                                                 activation_52[0][0]              \n",
      "                                                                 activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 17, 17, 192)  196608      merge_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 17, 17, 192)  768         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 17, 17, 192)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 17, 17, 192)  258048      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 17, 17, 192)  768         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 17, 17, 192)  0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 17, 17, 192)  196608      merge_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 17, 17, 224)  301056      activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 17, 17, 192)  768         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 17, 17, 224)  896         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 17, 17, 192)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 17, 17, 224)  0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 17, 17, 224)  301056      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 17, 17, 224)  351232      activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 17, 17, 224)  896         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 17, 17, 224)  896         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 17, 17, 224)  0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 17, 17, 224)  0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 17, 17, 1024) 0           merge_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 17, 17, 384)  393216      merge_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 17, 17, 256)  401408      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 17, 17, 256)  401408      activation_61[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 17, 17, 128)  131072      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 17, 17, 384)  1536        conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 17, 17, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 17, 17, 256)  1024        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 17, 17, 128)  512         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 17, 17, 384)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 17, 17, 256)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 17, 17, 256)  0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 17, 17, 128)  0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_10 (Merge)                (None, 17, 17, 1024) 0           activation_54[0][0]              \n",
      "                                                                 activation_57[0][0]              \n",
      "                                                                 activation_62[0][0]              \n",
      "                                                                 activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 17, 17, 192)  196608      merge_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 17, 17, 192)  768         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 17, 17, 192)  0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 17, 17, 192)  258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 17, 17, 192)  768         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 17, 17, 192)  0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 17, 17, 192)  196608      merge_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 17, 17, 224)  301056      activation_69[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 17, 17, 192)  768         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 17, 17, 224)  896         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 17, 17, 192)  0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 17, 17, 224)  0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 17, 17, 224)  301056      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 17, 17, 224)  351232      activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 17, 17, 224)  896         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 17, 17, 224)  896         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 17, 17, 224)  0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 17, 17, 224)  0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 17, 17, 1024) 0           merge_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 17, 17, 384)  393216      merge_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 17, 17, 256)  401408      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 17, 17, 256)  401408      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 17, 17, 128)  131072      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 17, 17, 384)  1536        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 17, 17, 256)  1024        conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 17, 17, 256)  1024        conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 17, 17, 128)  512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 17, 17, 384)  0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 17, 17, 256)  0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 17, 17, 256)  0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 17, 17, 128)  0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_11 (Merge)                (None, 17, 17, 1024) 0           activation_64[0][0]              \n",
      "                                                                 activation_67[0][0]              \n",
      "                                                                 activation_72[0][0]              \n",
      "                                                                 activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 17, 17, 192)  196608      merge_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 17, 17, 192)  768         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 17, 17, 192)  0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 17, 17, 192)  258048      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 17, 17, 192)  768         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 17, 17, 192)  0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 17, 17, 192)  196608      merge_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 17, 17, 224)  301056      activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 17, 17, 192)  768         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 17, 17, 224)  896         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 17, 17, 192)  0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 17, 17, 224)  0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 17, 17, 224)  301056      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 17, 17, 224)  351232      activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 17, 17, 224)  896         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 17, 17, 224)  896         conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 17, 17, 224)  0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 17, 17, 224)  0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 17, 17, 1024) 0           merge_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 17, 17, 384)  393216      merge_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 17, 17, 256)  401408      activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 17, 17, 256)  401408      activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 17, 17, 128)  131072      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 17, 17, 384)  1536        conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 17, 17, 256)  1024        conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 17, 17, 256)  1024        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 17, 17, 128)  512         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 17, 17, 384)  0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 17, 17, 256)  0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 17, 17, 256)  0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 17, 17, 128)  0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_12 (Merge)                (None, 17, 17, 1024) 0           activation_74[0][0]              \n",
      "                                                                 activation_77[0][0]              \n",
      "                                                                 activation_82[0][0]              \n",
      "                                                                 activation_83[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 17, 17, 192)  196608      merge_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 17, 17, 192)  768         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 17, 17, 192)  0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 17, 17, 192)  258048      activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 17, 17, 192)  768         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 17, 17, 192)  0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 17, 17, 192)  196608      merge_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 17, 17, 224)  301056      activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 17, 17, 192)  768         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 17, 17, 224)  896         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 17, 17, 192)  0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 17, 17, 224)  0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 17, 17, 224)  301056      activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 17, 17, 224)  351232      activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 17, 17, 224)  896         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 17, 17, 224)  896         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 17, 17, 224)  0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 17, 17, 224)  0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 17, 17, 1024) 0           merge_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 17, 17, 384)  393216      merge_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 17, 17, 256)  401408      activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 17, 17, 256)  401408      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 17, 17, 128)  131072      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 17, 17, 384)  1536        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 17, 17, 256)  1024        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 17, 17, 256)  1024        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 17, 17, 128)  512         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 17, 17, 384)  0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 17, 17, 256)  0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 17, 17, 256)  0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 17, 17, 128)  0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "merge_13 (Merge)                (None, 17, 17, 1024) 0           activation_84[0][0]              \n",
      "                                                                 activation_87[0][0]              \n",
      "                                                                 activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 17, 17, 192)  196608      merge_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 17, 17, 192)  768         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 17, 17, 192)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 17, 17, 192)  258048      activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 17, 17, 192)  768         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 17, 17, 192)  0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 17, 17, 192)  196608      merge_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 17, 17, 224)  301056      activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 17, 17, 192)  768         conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 17, 17, 224)  896         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 17, 17, 192)  0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 17, 17, 224)  0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 17, 17, 224)  301056      activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 17, 17, 224)  351232      activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 17, 17, 224)  896         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 17, 17, 224)  896         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 17, 17, 224)  0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 17, 17, 224)  0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 17, 17, 1024) 0           merge_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 17, 17, 384)  393216      merge_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 17, 17, 256)  401408      activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 17, 17, 256)  401408      activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 17, 17, 128)  131072      average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 17, 17, 384)  1536        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 17, 17, 256)  1024        conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 17, 17, 256)  1024        conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 17, 17, 128)  512         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 17, 17, 384)  0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 17, 17, 256)  0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 17, 17, 256)  0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 17, 17, 128)  0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_14 (Merge)                (None, 17, 17, 1024) 0           activation_94[0][0]              \n",
      "                                                                 activation_97[0][0]              \n",
      "                                                                 activation_102[0][0]             \n",
      "                                                                 activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 17, 17, 192)  196608      merge_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 17, 17, 192)  768         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 17, 17, 192)  0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 17, 17, 192)  258048      activation_108[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 17, 17, 192)  768         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 17, 17, 192)  0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 17, 17, 192)  196608      merge_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 17, 17, 224)  301056      activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 17, 17, 192)  768         conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 17, 17, 224)  896         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 17, 17, 192)  0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 17, 17, 224)  0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 17, 17, 224)  301056      activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 17, 17, 224)  351232      activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 17, 17, 224)  896         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 17, 17, 224)  896         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 17, 17, 224)  0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 17, 17, 224)  0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 17, 17, 1024) 0           merge_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 17, 17, 384)  393216      merge_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 17, 17, 256)  401408      activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 17, 17, 256)  401408      activation_111[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 17, 17, 128)  131072      average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 17, 17, 384)  1536        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 17, 17, 256)  1024        conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 17, 17, 256)  1024        conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 17, 17, 128)  512         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 17, 17, 384)  0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 17, 17, 256)  0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 17, 17, 256)  0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 17, 17, 128)  0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_15 (Merge)                (None, 17, 17, 1024) 0           activation_104[0][0]             \n",
      "                                                                 activation_107[0][0]             \n",
      "                                                                 activation_112[0][0]             \n",
      "                                                                 activation_113[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 17, 17, 256)  262144      merge_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 17, 17, 256)  1024        conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 17, 17, 256)  0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 17, 17, 256)  458752      activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 17, 17, 256)  1024        conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 17, 17, 256)  0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 17, 17, 192)  196608      merge_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 17, 17, 320)  573440      activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 17, 17, 192)  768         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 17, 17, 320)  1280        conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 17, 17, 192)  0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 17, 17, 320)  0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 8, 8, 192)    331776      activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 8, 8, 320)    921600      activation_118[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 8, 8, 192)    768         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 8, 8, 320)    1280        conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 8, 8, 192)    0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 8, 8, 320)    0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 8, 8, 1024)   0           merge_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "merge_16 (Merge)                (None, 8, 8, 1536)   0           activation_115[0][0]             \n",
      "                                                                 activation_119[0][0]             \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 8, 8, 384)    589824      merge_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 8, 8, 384)    1536        conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 8, 8, 384)    0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 8, 8, 448)    516096      activation_124[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 8, 8, 448)    1792        conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 8, 8, 448)    0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 8, 8, 384)    589824      merge_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 8, 8, 512)    688128      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 8, 8, 384)    1536        conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 8, 8, 512)    2048        conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 8, 8, 384)    0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 8, 8, 512)    0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 8, 8, 256)    294912      activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 8, 8, 256)    294912      activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 8, 8, 256)    393216      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 8, 8, 256)    393216      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 8, 8, 1536)   0           merge_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 8, 8, 256)    393216      merge_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 8, 8, 256)    1024        conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 8, 8, 256)    1024        conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 8, 8, 256)    1024        conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 8, 8, 256)    1024        conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 8, 8, 256)    393216      average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 8, 8, 256)    1024        conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 8, 8, 256)    0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 8, 8, 256)    0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 8, 8, 256)    0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 8, 8, 256)    0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 8, 8, 256)    1024        conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 8, 8, 256)    0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_17 (Merge)                (None, 8, 8, 512)    0           activation_122[0][0]             \n",
      "                                                                 activation_123[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merge_18 (Merge)                (None, 8, 8, 512)    0           activation_127[0][0]             \n",
      "                                                                 activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 8, 8, 256)    0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_19 (Merge)                (None, 8, 8, 1536)   0           activation_120[0][0]             \n",
      "                                                                 merge_17[0][0]                   \n",
      "                                                                 merge_18[0][0]                   \n",
      "                                                                 activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 8, 8, 384)    589824      merge_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 8, 8, 384)    1536        conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 8, 8, 384)    0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 8, 8, 448)    516096      activation_134[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 8, 8, 448)    1792        conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 8, 8, 448)    0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 8, 8, 384)    589824      merge_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 8, 8, 512)    688128      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 8, 8, 384)    1536        conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 8, 8, 512)    2048        conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 8, 8, 384)    0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 8, 8, 512)    0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 8, 8, 256)    294912      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 8, 8, 256)    294912      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 8, 8, 256)    393216      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 8, 8, 256)    393216      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 8, 8, 1536)   0           merge_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 8, 8, 256)    393216      merge_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 8, 8, 256)    1024        conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 8, 8, 256)    1024        conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 8, 8, 256)    1024        conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 8, 8, 256)    1024        conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 8, 8, 256)    393216      average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 8, 8, 256)    1024        conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 8, 8, 256)    0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 8, 8, 256)    0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 8, 8, 256)    0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 8, 8, 256)    0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 8, 8, 256)    1024        conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 8, 8, 256)    0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_20 (Merge)                (None, 8, 8, 512)    0           activation_132[0][0]             \n",
      "                                                                 activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merge_21 (Merge)                (None, 8, 8, 512)    0           activation_137[0][0]             \n",
      "                                                                 activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 8, 8, 256)    0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_22 (Merge)                (None, 8, 8, 1536)   0           activation_130[0][0]             \n",
      "                                                                 merge_20[0][0]                   \n",
      "                                                                 merge_21[0][0]                   \n",
      "                                                                 activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 8, 8, 384)    589824      merge_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 8, 8, 384)    1536        conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 8, 8, 384)    0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 8, 8, 448)    516096      activation_144[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 8, 8, 448)    1792        conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 8, 8, 448)    0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 8, 8, 384)    589824      merge_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 8, 8, 512)    688128      activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 8, 8, 384)    1536        conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 8, 8, 512)    2048        conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 8, 8, 384)    0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 8, 8, 512)    0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 8, 8, 256)    294912      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 8, 8, 256)    294912      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 8, 8, 256)    393216      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 8, 8, 256)    393216      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 8, 8, 1536)   0           merge_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 8, 8, 256)    393216      merge_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 8, 8, 256)    1024        conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 8, 8, 256)    1024        conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 8, 8, 256)    1024        conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 8, 8, 256)    1024        conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 8, 8, 256)    393216      average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 8, 8, 256)    1024        conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 8, 8, 256)    0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 8, 8, 256)    0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 8, 8, 256)    0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 8, 8, 256)    0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 8, 8, 256)    1024        conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 8, 8, 256)    0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_23 (Merge)                (None, 8, 8, 512)    0           activation_142[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "merge_24 (Merge)                (None, 8, 8, 512)    0           activation_147[0][0]             \n",
      "                                                                 activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 8, 8, 256)    0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "merge_25 (Merge)                (None, 8, 8, 1536)   0           activation_140[0][0]             \n",
      "                                                                 merge_23[0][0]                   \n",
      "                                                                 merge_24[0][0]                   \n",
      "                                                                 activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, 1, 1, 1536)   0           merge_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 1, 1536)   0           average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1536)         0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1001)         1538537     flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 42,744,521\n",
      "Trainable params: 42,681,353\n",
      "Non-trainable params: 63,168\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##########CREATE MODEL INCEPTIONResNet V2#######################################\n",
    "from keras.layers import Input, merge, Dropout, Dense, Flatten, Activation\n",
    "from keras.layers.convolutional import MaxPooling2D, Convolution2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "\"\"\"\n",
    "Implementation of Inception Network v4 [Inception Network v4 Paper](http://arxiv.org/pdf/1602.07261v1.pdf) in Keras.\n",
    "\"\"\"\n",
    "\n",
    "TH_BACKEND_TH_DIM_ORDERING = \"https://github.com/titu1994/Inception-v4/releases/download/v1.2/inception_v4_weights_th_dim_ordering_th_kernels.h5\"\n",
    "TH_BACKEND_TF_DIM_ORDERING = \"https://github.com/titu1994/Inception-v4/releases/download/v1.2/inception_v4_weights_tf_dim_ordering_th_kernels.h5\"\n",
    "TF_BACKEND_TF_DIM_ORDERING = \"https://github.com/titu1994/Inception-v4/releases/download/v1.2/inception_v4_weights_tf_dim_ordering_tf_kernels.h5\"\n",
    "TF_BACKEND_TH_DIM_ORDERING = \"https://github.com/titu1994/Inception-v4/releases/download/v1.2/inception_v4_weights_th_dim_ordering_tf_kernels.h5\"\n",
    "\n",
    "\n",
    "def conv_block(x, nb_filter, nb_row, nb_col, border_mode='same', subsample=(1, 1), bias=False):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    x = Convolution2D(nb_filter, nb_row, nb_col, subsample=subsample, border_mode=border_mode, bias=bias)(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def inception_stem(input):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    # Input Shape is 299 x 299 x 3 (th) or 3 x 299 x 299 (th)\n",
    "    x = conv_block(input, 32, 3, 3, subsample=(2, 2), border_mode='valid')\n",
    "    x = conv_block(x, 32, 3, 3, border_mode='valid')\n",
    "    x = conv_block(x, 64, 3, 3)\n",
    "\n",
    "    x1 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='valid')(x)\n",
    "    x2 = conv_block(x, 96, 3, 3, subsample=(2, 2), border_mode='valid')\n",
    "\n",
    "    x = merge([x1, x2], mode='concat', concat_axis=channel_axis)\n",
    "\n",
    "    x1 = conv_block(x, 64, 1, 1)\n",
    "    x1 = conv_block(x1, 96, 3, 3, border_mode='valid')\n",
    "\n",
    "    x2 = conv_block(x, 64, 1, 1)\n",
    "    x2 = conv_block(x2, 64, 1, 7)\n",
    "    x2 = conv_block(x2, 64, 7, 1)\n",
    "    x2 = conv_block(x2, 96, 3, 3, border_mode='valid')\n",
    "\n",
    "    x = merge([x1, x2], mode='concat', concat_axis=channel_axis)\n",
    "\n",
    "    x1 = conv_block(x, 192, 3, 3, subsample=(2, 2), border_mode='valid')\n",
    "    x2 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='valid')(x)\n",
    "\n",
    "    x = merge([x1, x2], mode='concat', concat_axis=channel_axis)\n",
    "    return x\n",
    "\n",
    "\n",
    "def inception_A(input):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    a1 = conv_block(input, 96, 1, 1)\n",
    "\n",
    "    a2 = conv_block(input, 64, 1, 1)\n",
    "    a2 = conv_block(a2, 96, 3, 3)\n",
    "\n",
    "    a3 = conv_block(input, 64, 1, 1)\n",
    "    a3 = conv_block(a3, 96, 3, 3)\n",
    "    a3 = conv_block(a3, 96, 3, 3)\n",
    "\n",
    "    a4 = AveragePooling2D((3, 3), strides=(1, 1), border_mode='same')(input)\n",
    "    a4 = conv_block(a4, 96, 1, 1)\n",
    "\n",
    "    m = merge([a1, a2, a3, a4], mode='concat', concat_axis=channel_axis)\n",
    "    return m\n",
    "\n",
    "\n",
    "def inception_B(input):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    b1 = conv_block(input, 384, 1, 1)\n",
    "\n",
    "    b2 = conv_block(input, 192, 1, 1)\n",
    "    b2 = conv_block(b2, 224, 1, 7)\n",
    "    b2 = conv_block(b2, 256, 7, 1)\n",
    "\n",
    "    b3 = conv_block(input, 192, 1, 1)\n",
    "    b3 = conv_block(b3, 192, 7, 1)\n",
    "    b3 = conv_block(b3, 224, 1, 7)\n",
    "    b3 = conv_block(b3, 224, 7, 1)\n",
    "    b3 = conv_block(b3, 256, 1, 7)\n",
    "\n",
    "    b4 = AveragePooling2D((3, 3), strides=(1, 1), border_mode='same')(input)\n",
    "    b4 = conv_block(b4, 128, 1, 1)\n",
    "\n",
    "    m = merge([b1, b2, b3, b4], mode='concat', concat_axis=channel_axis)\n",
    "    return m\n",
    "\n",
    "\n",
    "def inception_C(input):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    c1 = conv_block(input, 256, 1, 1)\n",
    "\n",
    "    c2 = conv_block(input, 384, 1, 1)\n",
    "    c2_1 = conv_block(c2, 256, 1, 3)\n",
    "    c2_2 = conv_block(c2, 256, 3, 1)\n",
    "    c2 = merge([c2_1, c2_2], mode='concat', concat_axis=channel_axis)\n",
    "\n",
    "    c3 = conv_block(input, 384, 1, 1)\n",
    "    c3 = conv_block(c3, 448, 3, 1)\n",
    "    c3 = conv_block(c3, 512, 1, 3)\n",
    "    c3_1 = conv_block(c3, 256, 1, 3)\n",
    "    c3_2 = conv_block(c3, 256, 3, 1)\n",
    "    c3 = merge([c3_1, c3_2], mode='concat', concat_axis=channel_axis)\n",
    "\n",
    "    c4 = AveragePooling2D((3, 3), strides=(1, 1), border_mode='same')(input)\n",
    "    c4 = conv_block(c4, 256, 1, 1)\n",
    "\n",
    "    m = merge([c1, c2, c3, c4], mode='concat', concat_axis=channel_axis)\n",
    "    return m\n",
    "\n",
    "\n",
    "def reduction_A(input):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    r1 = conv_block(input, 384, 3, 3, subsample=(2, 2), border_mode='valid')\n",
    "\n",
    "    r2 = conv_block(input, 192, 1, 1)\n",
    "    r2 = conv_block(r2, 224, 3, 3)\n",
    "    r2 = conv_block(r2, 256, 3, 3, subsample=(2, 2), border_mode='valid')\n",
    "\n",
    "    r3 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='valid')(input)\n",
    "\n",
    "    m = merge([r1, r2, r3], mode='concat', concat_axis=channel_axis)\n",
    "    return m\n",
    "\n",
    "\n",
    "def reduction_B(input):\n",
    "    if K.image_dim_ordering() == \"th\":\n",
    "        channel_axis = 1\n",
    "    else:\n",
    "        channel_axis = -1\n",
    "\n",
    "    r1 = conv_block(input, 192, 1, 1)\n",
    "    r1 = conv_block(r1, 192, 3, 3, subsample=(2, 2), border_mode='valid')\n",
    "\n",
    "    r2 = conv_block(input, 256, 1, 1)\n",
    "    r2 = conv_block(r2, 256, 1, 7)\n",
    "    r2 = conv_block(r2, 320, 7, 1)\n",
    "    r2 = conv_block(r2, 320, 3, 3, subsample=(2, 2), border_mode='valid')\n",
    "\n",
    "    r3 = MaxPooling2D((3, 3), strides=(2, 2), border_mode='valid')(input)\n",
    "\n",
    "    m = merge([r1, r2, r3], mode='concat', concat_axis=channel_axis)\n",
    "    return m\n",
    "\n",
    "\n",
    "def create_inception_v4(nb_classes=1001, load_weights=True):\n",
    "    '''\n",
    "    Creates a inception v4 network\n",
    "\n",
    "    :param nb_classes: number of classes.txt\n",
    "    :return: Keras Model with 1 input and 1 output\n",
    "    '''\n",
    "\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        init = Input((3, 299, 299))\n",
    "    else:\n",
    "        init = Input((299, 299, 3))\n",
    "\n",
    "    # Input Shape is 299 x 299 x 3 (tf) or 3 x 299 x 299 (th)\n",
    "    x = inception_stem(init)\n",
    "\n",
    "    # 4 x Inception A\n",
    "    for i in range(4):\n",
    "        x = inception_A(x)\n",
    "\n",
    "    # Reduction A\n",
    "    x = reduction_A(x)\n",
    "\n",
    "    # 7 x Inception B\n",
    "    for i in range(7):\n",
    "        x = inception_B(x)\n",
    "\n",
    "    # Reduction B\n",
    "    x = reduction_B(x)\n",
    "\n",
    "    # 3 x Inception C\n",
    "    for i in range(3):\n",
    "        x = inception_C(x)\n",
    "\n",
    "    # Average Pooling\n",
    "    x = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "    # Dropout\n",
    "    x = Dropout(0.8)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    # Output\n",
    "    out = Dense(output_dim=nb_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(init, out, name='Inception-v4')\n",
    "\n",
    "    if load_weights:\n",
    "        if K.backend() == \"theano\":\n",
    "            if K.image_dim_ordering() == \"th\":\n",
    "                weights = get_file('inception_v4_weights_th_dim_ordering_th_kernels.h5', TH_BACKEND_TH_DIM_ORDERING,\n",
    "                                   cache_subdir='models')\n",
    "            else:\n",
    "                weights = get_file('inception_v4_weights_tf_dim_ordering_th_kernels.h5', TH_BACKEND_TF_DIM_ORDERING,\n",
    "                                   cache_subdir='models')\n",
    "        else:\n",
    "            if K.image_dim_ordering() == \"th\":\n",
    "                weights = get_file('inception_v4_weights_th_dim_ordering_tf_kernels.h5', TF_BACKEND_TH_DIM_ORDERING,\n",
    "                                   cache_subdir='models')\n",
    "            else:\n",
    "                weights = get_file('inception_v4_weights_tf_dim_ordering_tf_kernels.h5', TH_BACKEND_TF_DIM_ORDERING,\n",
    "                                   cache_subdir='models')\n",
    "\n",
    "        model.load_weights(weights)\n",
    "        print(\"Model weights loaded.\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from keras.utils.visualize_util import plot\n",
    "\n",
    "    inception_v4 = create_inception_v4(load_weights=True)\n",
    "    inception_v4.summary()\n",
    "\n",
    "    # plot(inception_v4, to_file=\"Inception-v4.png\", show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image  import ImageDataGenerator, img_to_array, load_img \n",
    "from keras.models  import Sequential\n",
    "from keras.layers  import Dropout,  Flatten,  Dense\n",
    "from keras.applications import InceptionV3\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical \n",
    "import time\n",
    "import os\n",
    "import numpy as np \n",
    "import math\n",
    "import  matplotlib.pyplot  as  plt\n",
    "\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "batch_size = 64 \n",
    "epochs = 80\n",
    "nb_train = 4000\n",
    "nb_validation = 1200\n",
    "nb_test = 1200\n",
    "nb_classes = 4\n",
    "nb_FC = 1024\n",
    "\n",
    "PATH_TO_FE = \"Inception\" \n",
    "train_dir  =  \"balanced_dataset/train\"\n",
    "validation_dir = \"balanced_dataset/validation\" \n",
    "test_dir  =  \"balanced_dataset/test\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 4 classes.\n",
      "Found 1200 images belonging to 4 classes.\n",
      "Found 1200 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING + FEATURE EXTRACTION\n",
    "\n",
    "datagen  =  ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory( \n",
    "    train_dir,\n",
    "    target_size = (img_width, img_height), \n",
    "    batch_size = batch_size,\n",
    "    class_mode  =  None,  \n",
    "    shuffle = False, \n",
    "    interpolation  =  'lanczos')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory( \n",
    "    validation_dir, \n",
    "    target_size=(img_width, img_height), \n",
    "    batch_size=batch_size, \n",
    "    class_mode=None,\n",
    "    shuffle=False, \n",
    "    interpolation  =  'lanczos')\n",
    "\n",
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size = (img_width, img_height),\n",
    "    batch_size = batch_size,\n",
    "    class_mode = None,\n",
    "    shuffle=False, \n",
    "    interpolation  =  'lanczos')\n",
    "\n",
    "###########Feature Extraction (FE)#########################\n",
    "#Train\n",
    "max_size_train = int(math.ceil(nb_train / batch_size)) \n",
    "train_features = inception_v4.predict_generator(train_generator, max_size_train)\n",
    "np.save('InceptionV4/InceptionV4_FE_train_1.npy',  train_features)\n",
    "#Validation\n",
    "max_size_validation = int(math.ceil(nb_validation / batch_size)) \n",
    "validation_features  =  inception_v4.predict_generator(validation_generator,  max_size_validation) \n",
    "np.save('InceptionV4/InceptionV4_FE_validation_1.npy',  validation_features)\n",
    "#Test\n",
    "max_size_test = int(math.ceil(nb_test / batch_size)) \n",
    "test_features  =  inception_v4.predict_generator(test_generator,  max_size_test)\n",
    "\n",
    "#Labels Extraction\n",
    "train_labels  =  train_generator.classes  \n",
    "train_labels = to_categorical(train_labels, num_classes=nb_classes) \n",
    "validation_labels  =  validation_generator.classes  \n",
    "validation_labels  =  to_categorical(validation_labels, num_classes=nb_classes) \n",
    "test_labels  =  test_generator.classes  \n",
    "test_labels  =  to_categorical(test_labels, num_classes=nb_classes) \n",
    "\n",
    "#Save Features\n",
    "train_data = np.load('InceptionV4/InceptionV4_FE_train_1.npy') \n",
    "validation_data  =  np.load('InceptionV4/InceptionV4_FE_validation_1.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top Model Classificator\n",
    "model = Sequential() \n",
    "#model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(nb_FC,  activation='relu', input_shape=train_data.shape[1:])) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes,  activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile models\n",
    "model.compile(\n",
    "    optimizer = RMSprop(lr=2e-5),\n",
    "    loss='categorical_crossentropy',  \n",
    "    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1200 samples\n",
      "Epoch 1/1000\n",
      "4000/4000 [==============================] - 0s 70us/step - loss: 0.5992 - acc: 0.7710 - val_loss: 0.8203 - val_acc: 0.7117\n",
      "Epoch 2/1000\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.6035 - acc: 0.7702 - val_loss: 0.8204 - val_acc: 0.7092\n",
      "Epoch 3/1000\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.5980 - acc: 0.7732 - val_loss: 0.8211 - val_acc: 0.7108\n",
      "Epoch 4/1000\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.6048 - acc: 0.7700 - val_loss: 0.8202 - val_acc: 0.7100\n",
      "Epoch 5/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6007 - acc: 0.7655 - val_loss: 0.8200 - val_acc: 0.7142\n",
      "Epoch 6/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.6007 - acc: 0.7712 - val_loss: 0.8206 - val_acc: 0.7142\n",
      "Epoch 7/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5917 - acc: 0.7777 - val_loss: 0.8205 - val_acc: 0.7108\n",
      "Epoch 8/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5963 - acc: 0.7745 - val_loss: 0.8206 - val_acc: 0.7117\n",
      "Epoch 9/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6035 - acc: 0.7687 - val_loss: 0.8212 - val_acc: 0.7108\n",
      "Epoch 10/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6017 - acc: 0.7712 - val_loss: 0.8207 - val_acc: 0.7125\n",
      "Epoch 11/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5949 - acc: 0.7720 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 12/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5986 - acc: 0.7717 - val_loss: 0.8203 - val_acc: 0.7117\n",
      "Epoch 13/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5997 - acc: 0.7695 - val_loss: 0.8195 - val_acc: 0.7108\n",
      "Epoch 14/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6018 - acc: 0.7695 - val_loss: 0.8192 - val_acc: 0.7117\n",
      "Epoch 15/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6002 - acc: 0.7725 - val_loss: 0.8201 - val_acc: 0.7108\n",
      "Epoch 16/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5951 - acc: 0.7755 - val_loss: 0.8197 - val_acc: 0.7125\n",
      "Epoch 17/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5986 - acc: 0.7695 - val_loss: 0.8208 - val_acc: 0.7092\n",
      "Epoch 18/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5971 - acc: 0.7730 - val_loss: 0.8204 - val_acc: 0.7125\n",
      "Epoch 19/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5960 - acc: 0.7738 - val_loss: 0.8205 - val_acc: 0.7117\n",
      "Epoch 20/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5999 - acc: 0.7775 - val_loss: 0.8198 - val_acc: 0.7142\n",
      "Epoch 21/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6012 - acc: 0.7692 - val_loss: 0.8194 - val_acc: 0.7125\n",
      "Epoch 22/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5941 - acc: 0.7690 - val_loss: 0.8202 - val_acc: 0.7142\n",
      "Epoch 23/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6022 - acc: 0.7700 - val_loss: 0.8200 - val_acc: 0.7133\n",
      "Epoch 24/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5939 - acc: 0.7788 - val_loss: 0.8208 - val_acc: 0.7108\n",
      "Epoch 25/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5978 - acc: 0.7738 - val_loss: 0.8206 - val_acc: 0.7100\n",
      "Epoch 26/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5981 - acc: 0.7740 - val_loss: 0.8202 - val_acc: 0.7108\n",
      "Epoch 27/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5921 - acc: 0.7762 - val_loss: 0.8212 - val_acc: 0.7117\n",
      "Epoch 28/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.6001 - acc: 0.7712 - val_loss: 0.8205 - val_acc: 0.7108\n",
      "Epoch 29/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6062 - acc: 0.7657 - val_loss: 0.8199 - val_acc: 0.7133\n",
      "Epoch 30/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5943 - acc: 0.7685 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 31/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.6031 - acc: 0.7717 - val_loss: 0.8202 - val_acc: 0.7117\n",
      "Epoch 32/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5961 - acc: 0.7698 - val_loss: 0.8210 - val_acc: 0.7100\n",
      "Epoch 33/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6042 - acc: 0.7710 - val_loss: 0.8211 - val_acc: 0.7108\n",
      "Epoch 34/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6085 - acc: 0.7670 - val_loss: 0.8201 - val_acc: 0.7133\n",
      "Epoch 35/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5954 - acc: 0.7730 - val_loss: 0.8203 - val_acc: 0.7125\n",
      "Epoch 36/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5980 - acc: 0.7710 - val_loss: 0.8200 - val_acc: 0.7133\n",
      "Epoch 37/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5991 - acc: 0.7668 - val_loss: 0.8200 - val_acc: 0.7142\n",
      "Epoch 38/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5961 - acc: 0.7728 - val_loss: 0.8201 - val_acc: 0.7117\n",
      "Epoch 39/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5994 - acc: 0.7675 - val_loss: 0.8205 - val_acc: 0.7108\n",
      "Epoch 40/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5975 - acc: 0.7735 - val_loss: 0.8206 - val_acc: 0.7117\n",
      "Epoch 41/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5986 - acc: 0.7690 - val_loss: 0.8199 - val_acc: 0.7125\n",
      "Epoch 42/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5922 - acc: 0.7755 - val_loss: 0.8200 - val_acc: 0.7108\n",
      "Epoch 43/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5987 - acc: 0.7712 - val_loss: 0.8196 - val_acc: 0.7117\n",
      "Epoch 44/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6024 - acc: 0.7710 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 45/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5974 - acc: 0.7725 - val_loss: 0.8202 - val_acc: 0.7108\n",
      "Epoch 46/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5979 - acc: 0.7690 - val_loss: 0.8200 - val_acc: 0.7117\n",
      "Epoch 47/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6025 - acc: 0.7675 - val_loss: 0.8206 - val_acc: 0.7125\n",
      "Epoch 48/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5954 - acc: 0.7685 - val_loss: 0.8199 - val_acc: 0.7117\n",
      "Epoch 49/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6000 - acc: 0.7728 - val_loss: 0.8190 - val_acc: 0.7142\n",
      "Epoch 50/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6039 - acc: 0.7675 - val_loss: 0.8196 - val_acc: 0.7100\n",
      "Epoch 51/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6052 - acc: 0.7755 - val_loss: 0.8201 - val_acc: 0.7117\n",
      "Epoch 52/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5977 - acc: 0.7755 - val_loss: 0.8197 - val_acc: 0.7108\n",
      "Epoch 53/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5907 - acc: 0.7720 - val_loss: 0.8202 - val_acc: 0.7117\n",
      "Epoch 54/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5963 - acc: 0.7712 - val_loss: 0.8198 - val_acc: 0.7100\n",
      "Epoch 55/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6012 - acc: 0.7692 - val_loss: 0.8197 - val_acc: 0.7158\n",
      "Epoch 56/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5972 - acc: 0.7717 - val_loss: 0.8207 - val_acc: 0.7100\n",
      "Epoch 57/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5983 - acc: 0.7708 - val_loss: 0.8198 - val_acc: 0.7125\n",
      "Epoch 58/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5979 - acc: 0.7745 - val_loss: 0.8199 - val_acc: 0.7125\n",
      "Epoch 59/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5979 - acc: 0.7682 - val_loss: 0.8196 - val_acc: 0.7117\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5998 - acc: 0.7705 - val_loss: 0.8197 - val_acc: 0.7133\n",
      "Epoch 61/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5957 - acc: 0.7685 - val_loss: 0.8201 - val_acc: 0.7100\n",
      "Epoch 62/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5963 - acc: 0.7722 - val_loss: 0.8209 - val_acc: 0.7125\n",
      "Epoch 63/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5937 - acc: 0.7717 - val_loss: 0.8197 - val_acc: 0.7117\n",
      "Epoch 64/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5912 - acc: 0.7730 - val_loss: 0.8197 - val_acc: 0.7125\n",
      "Epoch 65/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5992 - acc: 0.7650 - val_loss: 0.8201 - val_acc: 0.7100\n",
      "Epoch 66/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5927 - acc: 0.7687 - val_loss: 0.8206 - val_acc: 0.7125\n",
      "Epoch 67/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5971 - acc: 0.7715 - val_loss: 0.8207 - val_acc: 0.7108\n",
      "Epoch 68/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5996 - acc: 0.7678 - val_loss: 0.8193 - val_acc: 0.7108\n",
      "Epoch 69/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6016 - acc: 0.7760 - val_loss: 0.8192 - val_acc: 0.7108\n",
      "Epoch 70/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6009 - acc: 0.7687 - val_loss: 0.8212 - val_acc: 0.7092\n",
      "Epoch 71/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5971 - acc: 0.7732 - val_loss: 0.8199 - val_acc: 0.7108\n",
      "Epoch 72/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.6009 - acc: 0.7690 - val_loss: 0.8207 - val_acc: 0.7125\n",
      "Epoch 73/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5961 - acc: 0.7735 - val_loss: 0.8196 - val_acc: 0.7125\n",
      "Epoch 74/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6061 - acc: 0.7687 - val_loss: 0.8200 - val_acc: 0.7100\n",
      "Epoch 75/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5968 - acc: 0.7745 - val_loss: 0.8206 - val_acc: 0.7125\n",
      "Epoch 76/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6008 - acc: 0.7732 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 77/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6034 - acc: 0.7670 - val_loss: 0.8201 - val_acc: 0.7100\n",
      "Epoch 78/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5956 - acc: 0.7752 - val_loss: 0.8197 - val_acc: 0.7133\n",
      "Epoch 79/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5985 - acc: 0.7758 - val_loss: 0.8197 - val_acc: 0.7108\n",
      "Epoch 80/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6009 - acc: 0.7692 - val_loss: 0.8199 - val_acc: 0.7117\n",
      "Epoch 81/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5982 - acc: 0.7728 - val_loss: 0.8203 - val_acc: 0.7117\n",
      "Epoch 82/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5983 - acc: 0.7692 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 83/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5979 - acc: 0.7760 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 84/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5916 - acc: 0.7725 - val_loss: 0.8204 - val_acc: 0.7117\n",
      "Epoch 85/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5926 - acc: 0.7732 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 86/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6010 - acc: 0.7708 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 87/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5932 - acc: 0.7672 - val_loss: 0.8201 - val_acc: 0.7108\n",
      "Epoch 88/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5988 - acc: 0.7777 - val_loss: 0.8211 - val_acc: 0.7125\n",
      "Epoch 89/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6030 - acc: 0.7675 - val_loss: 0.8209 - val_acc: 0.7100\n",
      "Epoch 90/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5941 - acc: 0.7755 - val_loss: 0.8207 - val_acc: 0.7125\n",
      "Epoch 91/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5984 - acc: 0.7687 - val_loss: 0.8207 - val_acc: 0.7117\n",
      "Epoch 92/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5906 - acc: 0.7715 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 93/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5994 - acc: 0.7670 - val_loss: 0.8200 - val_acc: 0.7133\n",
      "Epoch 94/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5914 - acc: 0.7758 - val_loss: 0.8202 - val_acc: 0.7142\n",
      "Epoch 95/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5942 - acc: 0.7747 - val_loss: 0.8194 - val_acc: 0.7142\n",
      "Epoch 96/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5962 - acc: 0.7665 - val_loss: 0.8200 - val_acc: 0.7117\n",
      "Epoch 97/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6022 - acc: 0.7665 - val_loss: 0.8203 - val_acc: 0.7117\n",
      "Epoch 98/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5982 - acc: 0.7712 - val_loss: 0.8202 - val_acc: 0.7125\n",
      "Epoch 99/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5927 - acc: 0.7692 - val_loss: 0.8210 - val_acc: 0.7100\n",
      "Epoch 100/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5945 - acc: 0.7700 - val_loss: 0.8195 - val_acc: 0.7142\n",
      "Epoch 101/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6005 - acc: 0.7698 - val_loss: 0.8193 - val_acc: 0.7133\n",
      "Epoch 102/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5956 - acc: 0.7720 - val_loss: 0.8206 - val_acc: 0.7125\n",
      "Epoch 103/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5949 - acc: 0.7715 - val_loss: 0.8208 - val_acc: 0.7108\n",
      "Epoch 104/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5987 - acc: 0.7750 - val_loss: 0.8209 - val_acc: 0.7117\n",
      "Epoch 105/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6010 - acc: 0.7725 - val_loss: 0.8196 - val_acc: 0.7133\n",
      "Epoch 106/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5948 - acc: 0.7722 - val_loss: 0.8202 - val_acc: 0.7108\n",
      "Epoch 107/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5940 - acc: 0.7738 - val_loss: 0.8199 - val_acc: 0.7133\n",
      "Epoch 108/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5967 - acc: 0.7702 - val_loss: 0.8202 - val_acc: 0.7108\n",
      "Epoch 109/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5971 - acc: 0.7722 - val_loss: 0.8196 - val_acc: 0.7125\n",
      "Epoch 110/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5903 - acc: 0.7785 - val_loss: 0.8196 - val_acc: 0.7133\n",
      "Epoch 111/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5972 - acc: 0.7722 - val_loss: 0.8197 - val_acc: 0.7108\n",
      "Epoch 112/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5966 - acc: 0.7760 - val_loss: 0.8197 - val_acc: 0.7100\n",
      "Epoch 113/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5935 - acc: 0.7732 - val_loss: 0.8202 - val_acc: 0.7108\n",
      "Epoch 114/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6016 - acc: 0.7692 - val_loss: 0.8202 - val_acc: 0.7117\n",
      "Epoch 115/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5946 - acc: 0.7755 - val_loss: 0.8199 - val_acc: 0.7108\n",
      "Epoch 116/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6001 - acc: 0.7678 - val_loss: 0.8194 - val_acc: 0.7133\n",
      "Epoch 117/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5986 - acc: 0.7687 - val_loss: 0.8202 - val_acc: 0.7117\n",
      "Epoch 118/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5986 - acc: 0.7690 - val_loss: 0.8195 - val_acc: 0.7150\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5970 - acc: 0.7745 - val_loss: 0.8192 - val_acc: 0.7117\n",
      "Epoch 120/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5971 - acc: 0.7738 - val_loss: 0.8204 - val_acc: 0.7133\n",
      "Epoch 121/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5968 - acc: 0.7708 - val_loss: 0.8202 - val_acc: 0.7117\n",
      "Epoch 122/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5904 - acc: 0.7807 - val_loss: 0.8203 - val_acc: 0.7108\n",
      "Epoch 123/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6039 - acc: 0.7690 - val_loss: 0.8198 - val_acc: 0.7125\n",
      "Epoch 124/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5961 - acc: 0.7705 - val_loss: 0.8197 - val_acc: 0.7125\n",
      "Epoch 125/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5952 - acc: 0.7750 - val_loss: 0.8196 - val_acc: 0.7125\n",
      "Epoch 126/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5977 - acc: 0.7710 - val_loss: 0.8200 - val_acc: 0.7108\n",
      "Epoch 127/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5942 - acc: 0.7682 - val_loss: 0.8199 - val_acc: 0.7100\n",
      "Epoch 128/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6019 - acc: 0.7705 - val_loss: 0.8196 - val_acc: 0.7125\n",
      "Epoch 129/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5950 - acc: 0.7755 - val_loss: 0.8209 - val_acc: 0.7133\n",
      "Epoch 130/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5967 - acc: 0.7760 - val_loss: 0.8199 - val_acc: 0.7125\n",
      "Epoch 131/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6001 - acc: 0.7715 - val_loss: 0.8204 - val_acc: 0.7133\n",
      "Epoch 132/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5942 - acc: 0.7735 - val_loss: 0.8194 - val_acc: 0.7142\n",
      "Epoch 133/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.6000 - acc: 0.7775 - val_loss: 0.8196 - val_acc: 0.7133\n",
      "Epoch 134/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5942 - acc: 0.7728 - val_loss: 0.8205 - val_acc: 0.7117\n",
      "Epoch 135/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5960 - acc: 0.7710 - val_loss: 0.8191 - val_acc: 0.7133\n",
      "Epoch 136/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5985 - acc: 0.7760 - val_loss: 0.8196 - val_acc: 0.7108\n",
      "Epoch 137/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5993 - acc: 0.7752 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 138/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5928 - acc: 0.7692 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 139/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5984 - acc: 0.7690 - val_loss: 0.8195 - val_acc: 0.7125\n",
      "Epoch 140/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5958 - acc: 0.7740 - val_loss: 0.8192 - val_acc: 0.7125\n",
      "Epoch 141/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5994 - acc: 0.7698 - val_loss: 0.8208 - val_acc: 0.7092\n",
      "Epoch 142/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5935 - acc: 0.7735 - val_loss: 0.8191 - val_acc: 0.7100\n",
      "Epoch 143/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5972 - acc: 0.7698 - val_loss: 0.8193 - val_acc: 0.7133\n",
      "Epoch 144/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6008 - acc: 0.7662 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 145/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5958 - acc: 0.7702 - val_loss: 0.8206 - val_acc: 0.7125\n",
      "Epoch 146/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6015 - acc: 0.7687 - val_loss: 0.8196 - val_acc: 0.7150\n",
      "Epoch 147/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5965 - acc: 0.7710 - val_loss: 0.8195 - val_acc: 0.7133\n",
      "Epoch 148/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5988 - acc: 0.7685 - val_loss: 0.8200 - val_acc: 0.7108\n",
      "Epoch 149/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5949 - acc: 0.7715 - val_loss: 0.8195 - val_acc: 0.7125\n",
      "Epoch 150/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5935 - acc: 0.7720 - val_loss: 0.8198 - val_acc: 0.7117\n",
      "Epoch 151/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5882 - acc: 0.7758 - val_loss: 0.8204 - val_acc: 0.7125\n",
      "Epoch 152/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5950 - acc: 0.7710 - val_loss: 0.8194 - val_acc: 0.7142\n",
      "Epoch 153/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5868 - acc: 0.7752 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 154/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5890 - acc: 0.7760 - val_loss: 0.8202 - val_acc: 0.7125\n",
      "Epoch 155/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5989 - acc: 0.7750 - val_loss: 0.8197 - val_acc: 0.7108\n",
      "Epoch 156/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6000 - acc: 0.7742 - val_loss: 0.8197 - val_acc: 0.7108\n",
      "Epoch 157/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6012 - acc: 0.7660 - val_loss: 0.8206 - val_acc: 0.7125\n",
      "Epoch 158/1000\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.5889 - acc: 0.7798 - val_loss: 0.8197 - val_acc: 0.7125\n",
      "Epoch 159/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5949 - acc: 0.7717 - val_loss: 0.8195 - val_acc: 0.7117\n",
      "Epoch 160/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5970 - acc: 0.7715 - val_loss: 0.8210 - val_acc: 0.7125\n",
      "Epoch 161/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5912 - acc: 0.7705 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 162/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5968 - acc: 0.7777 - val_loss: 0.8204 - val_acc: 0.7125\n",
      "Epoch 163/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5949 - acc: 0.7712 - val_loss: 0.8204 - val_acc: 0.7133\n",
      "Epoch 164/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6005 - acc: 0.7690 - val_loss: 0.8204 - val_acc: 0.7100\n",
      "Epoch 165/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5932 - acc: 0.7735 - val_loss: 0.8190 - val_acc: 0.7142\n",
      "Epoch 166/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5964 - acc: 0.7672 - val_loss: 0.8206 - val_acc: 0.7117\n",
      "Epoch 167/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5964 - acc: 0.7770 - val_loss: 0.8199 - val_acc: 0.7117\n",
      "Epoch 168/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5907 - acc: 0.7830 - val_loss: 0.8199 - val_acc: 0.7125\n",
      "Epoch 169/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5957 - acc: 0.7738 - val_loss: 0.8200 - val_acc: 0.7150\n",
      "Epoch 170/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5970 - acc: 0.7715 - val_loss: 0.8194 - val_acc: 0.7142\n",
      "Epoch 171/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5920 - acc: 0.7725 - val_loss: 0.8207 - val_acc: 0.7125\n",
      "Epoch 172/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5985 - acc: 0.7780 - val_loss: 0.8199 - val_acc: 0.7133\n",
      "Epoch 173/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5944 - acc: 0.7690 - val_loss: 0.8195 - val_acc: 0.7133\n",
      "Epoch 174/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5921 - acc: 0.7725 - val_loss: 0.8196 - val_acc: 0.7125\n",
      "Epoch 175/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5967 - acc: 0.7728 - val_loss: 0.8195 - val_acc: 0.7150\n",
      "Epoch 176/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5886 - acc: 0.7770 - val_loss: 0.8198 - val_acc: 0.7142\n",
      "Epoch 177/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5961 - acc: 0.7742 - val_loss: 0.8189 - val_acc: 0.7150\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5950 - acc: 0.7732 - val_loss: 0.8202 - val_acc: 0.7125\n",
      "Epoch 179/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5992 - acc: 0.7735 - val_loss: 0.8201 - val_acc: 0.7108\n",
      "Epoch 180/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5933 - acc: 0.7715 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 181/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5973 - acc: 0.7705 - val_loss: 0.8201 - val_acc: 0.7133\n",
      "Epoch 182/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5968 - acc: 0.7752 - val_loss: 0.8194 - val_acc: 0.7133\n",
      "Epoch 183/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5858 - acc: 0.7760 - val_loss: 0.8196 - val_acc: 0.7108\n",
      "Epoch 184/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5975 - acc: 0.7702 - val_loss: 0.8200 - val_acc: 0.7158\n",
      "Epoch 185/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5900 - acc: 0.7742 - val_loss: 0.8202 - val_acc: 0.7142\n",
      "Epoch 186/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5948 - acc: 0.7780 - val_loss: 0.8201 - val_acc: 0.7133\n",
      "Epoch 187/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5909 - acc: 0.7710 - val_loss: 0.8198 - val_acc: 0.7125\n",
      "Epoch 188/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5975 - acc: 0.7738 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 189/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5945 - acc: 0.7765 - val_loss: 0.8194 - val_acc: 0.7133\n",
      "Epoch 190/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5970 - acc: 0.7735 - val_loss: 0.8192 - val_acc: 0.7142\n",
      "Epoch 191/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5906 - acc: 0.7750 - val_loss: 0.8191 - val_acc: 0.7108\n",
      "Epoch 192/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5902 - acc: 0.7760 - val_loss: 0.8200 - val_acc: 0.7133\n",
      "Epoch 193/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5978 - acc: 0.7698 - val_loss: 0.8194 - val_acc: 0.7125\n",
      "Epoch 194/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5929 - acc: 0.7768 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 195/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5954 - acc: 0.7690 - val_loss: 0.8210 - val_acc: 0.7133\n",
      "Epoch 196/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5933 - acc: 0.7725 - val_loss: 0.8202 - val_acc: 0.7125\n",
      "Epoch 197/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5911 - acc: 0.7700 - val_loss: 0.8199 - val_acc: 0.7117\n",
      "Epoch 198/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5943 - acc: 0.7702 - val_loss: 0.8199 - val_acc: 0.7133\n",
      "Epoch 199/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.6005 - acc: 0.7685 - val_loss: 0.8201 - val_acc: 0.7117\n",
      "Epoch 200/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5936 - acc: 0.7692 - val_loss: 0.8199 - val_acc: 0.7125\n",
      "Epoch 201/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5936 - acc: 0.7732 - val_loss: 0.8202 - val_acc: 0.7133\n",
      "Epoch 202/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5950 - acc: 0.7705 - val_loss: 0.8200 - val_acc: 0.7150\n",
      "Epoch 203/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5941 - acc: 0.7725 - val_loss: 0.8203 - val_acc: 0.7125\n",
      "Epoch 204/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5936 - acc: 0.7730 - val_loss: 0.8192 - val_acc: 0.7125\n",
      "Epoch 205/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5972 - acc: 0.7720 - val_loss: 0.8196 - val_acc: 0.7133\n",
      "Epoch 206/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5918 - acc: 0.7760 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 207/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5906 - acc: 0.7722 - val_loss: 0.8193 - val_acc: 0.7133\n",
      "Epoch 208/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5967 - acc: 0.7717 - val_loss: 0.8203 - val_acc: 0.7142\n",
      "Epoch 209/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5910 - acc: 0.7717 - val_loss: 0.8201 - val_acc: 0.7142\n",
      "Epoch 210/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5932 - acc: 0.7720 - val_loss: 0.8206 - val_acc: 0.7117\n",
      "Epoch 211/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5932 - acc: 0.7762 - val_loss: 0.8208 - val_acc: 0.7117\n",
      "Epoch 212/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5888 - acc: 0.7712 - val_loss: 0.8199 - val_acc: 0.7142\n",
      "Epoch 213/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5954 - acc: 0.7740 - val_loss: 0.8203 - val_acc: 0.7142\n",
      "Epoch 214/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5987 - acc: 0.7678 - val_loss: 0.8196 - val_acc: 0.7150\n",
      "Epoch 215/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5965 - acc: 0.7672 - val_loss: 0.8203 - val_acc: 0.7117\n",
      "Epoch 216/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5908 - acc: 0.7747 - val_loss: 0.8187 - val_acc: 0.7108\n",
      "Epoch 217/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5909 - acc: 0.7687 - val_loss: 0.8193 - val_acc: 0.7117\n",
      "Epoch 218/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5980 - acc: 0.7698 - val_loss: 0.8197 - val_acc: 0.7117\n",
      "Epoch 219/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5941 - acc: 0.7735 - val_loss: 0.8188 - val_acc: 0.7125\n",
      "Epoch 220/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5942 - acc: 0.7740 - val_loss: 0.8189 - val_acc: 0.7133\n",
      "Epoch 221/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5972 - acc: 0.7712 - val_loss: 0.8185 - val_acc: 0.7125\n",
      "Epoch 222/1000\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.5909 - acc: 0.7722 - val_loss: 0.8193 - val_acc: 0.7133\n",
      "Epoch 223/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5853 - acc: 0.7740 - val_loss: 0.8196 - val_acc: 0.7125\n",
      "Epoch 224/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5969 - acc: 0.7678 - val_loss: 0.8203 - val_acc: 0.7108\n",
      "Epoch 225/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5946 - acc: 0.7710 - val_loss: 0.8197 - val_acc: 0.7100\n",
      "Epoch 226/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5945 - acc: 0.7745 - val_loss: 0.8208 - val_acc: 0.7117\n",
      "Epoch 227/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5866 - acc: 0.7772 - val_loss: 0.8202 - val_acc: 0.7125\n",
      "Epoch 228/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5946 - acc: 0.7722 - val_loss: 0.8191 - val_acc: 0.7142\n",
      "Epoch 229/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5980 - acc: 0.7702 - val_loss: 0.8189 - val_acc: 0.7133\n",
      "Epoch 230/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5976 - acc: 0.7742 - val_loss: 0.8201 - val_acc: 0.7133\n",
      "Epoch 231/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5949 - acc: 0.7732 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 232/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5994 - acc: 0.7695 - val_loss: 0.8195 - val_acc: 0.7100\n",
      "Epoch 233/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5916 - acc: 0.7722 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 234/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5959 - acc: 0.7740 - val_loss: 0.8203 - val_acc: 0.7133\n",
      "Epoch 235/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5926 - acc: 0.7728 - val_loss: 0.8198 - val_acc: 0.7117\n",
      "Epoch 236/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5889 - acc: 0.7717 - val_loss: 0.8192 - val_acc: 0.7125\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5929 - acc: 0.7750 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 238/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5938 - acc: 0.7745 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 239/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5882 - acc: 0.7758 - val_loss: 0.8204 - val_acc: 0.7125\n",
      "Epoch 240/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5918 - acc: 0.7728 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 241/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5935 - acc: 0.7768 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 242/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6012 - acc: 0.7732 - val_loss: 0.8200 - val_acc: 0.7150\n",
      "Epoch 243/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.6010 - acc: 0.7687 - val_loss: 0.8191 - val_acc: 0.7142\n",
      "Epoch 244/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5963 - acc: 0.7720 - val_loss: 0.8201 - val_acc: 0.7142\n",
      "Epoch 245/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5929 - acc: 0.7752 - val_loss: 0.8197 - val_acc: 0.7150\n",
      "Epoch 246/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5922 - acc: 0.7750 - val_loss: 0.8193 - val_acc: 0.7125\n",
      "Epoch 247/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5956 - acc: 0.7700 - val_loss: 0.8196 - val_acc: 0.7117\n",
      "Epoch 248/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5942 - acc: 0.7678 - val_loss: 0.8195 - val_acc: 0.7117\n",
      "Epoch 249/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5887 - acc: 0.7792 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 250/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5864 - acc: 0.7740 - val_loss: 0.8196 - val_acc: 0.7142\n",
      "Epoch 251/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5899 - acc: 0.7740 - val_loss: 0.8207 - val_acc: 0.7117\n",
      "Epoch 252/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5912 - acc: 0.7708 - val_loss: 0.8197 - val_acc: 0.7117\n",
      "Epoch 253/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5976 - acc: 0.7747 - val_loss: 0.8199 - val_acc: 0.7150\n",
      "Epoch 254/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5914 - acc: 0.7770 - val_loss: 0.8198 - val_acc: 0.7150\n",
      "Epoch 255/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5920 - acc: 0.7738 - val_loss: 0.8199 - val_acc: 0.7117\n",
      "Epoch 256/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5911 - acc: 0.7740 - val_loss: 0.8204 - val_acc: 0.7100\n",
      "Epoch 257/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5903 - acc: 0.7715 - val_loss: 0.8206 - val_acc: 0.7133\n",
      "Epoch 258/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5904 - acc: 0.7760 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 259/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5892 - acc: 0.7825 - val_loss: 0.8203 - val_acc: 0.7158\n",
      "Epoch 260/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5931 - acc: 0.7762 - val_loss: 0.8207 - val_acc: 0.7142\n",
      "Epoch 261/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5972 - acc: 0.7745 - val_loss: 0.8203 - val_acc: 0.7125\n",
      "Epoch 262/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5908 - acc: 0.7738 - val_loss: 0.8196 - val_acc: 0.7125\n",
      "Epoch 263/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5941 - acc: 0.7668 - val_loss: 0.8192 - val_acc: 0.7133\n",
      "Epoch 264/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5858 - acc: 0.7735 - val_loss: 0.8199 - val_acc: 0.7125\n",
      "Epoch 265/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5923 - acc: 0.7768 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 266/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5919 - acc: 0.7722 - val_loss: 0.8209 - val_acc: 0.7125\n",
      "Epoch 267/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5886 - acc: 0.7795 - val_loss: 0.8217 - val_acc: 0.7150\n",
      "Epoch 268/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5891 - acc: 0.7725 - val_loss: 0.8203 - val_acc: 0.7158\n",
      "Epoch 269/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5869 - acc: 0.7800 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 270/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5935 - acc: 0.7702 - val_loss: 0.8204 - val_acc: 0.7133\n",
      "Epoch 271/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5886 - acc: 0.7770 - val_loss: 0.8213 - val_acc: 0.7117\n",
      "Epoch 272/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5963 - acc: 0.7680 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 273/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5963 - acc: 0.7735 - val_loss: 0.8205 - val_acc: 0.7133\n",
      "Epoch 274/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5972 - acc: 0.7717 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 275/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5898 - acc: 0.7738 - val_loss: 0.8186 - val_acc: 0.7133\n",
      "Epoch 276/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5930 - acc: 0.7720 - val_loss: 0.8197 - val_acc: 0.7150\n",
      "Epoch 277/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5939 - acc: 0.7807 - val_loss: 0.8195 - val_acc: 0.7150\n",
      "Epoch 278/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5879 - acc: 0.7712 - val_loss: 0.8197 - val_acc: 0.7150\n",
      "Epoch 279/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5985 - acc: 0.7722 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 280/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5937 - acc: 0.7710 - val_loss: 0.8190 - val_acc: 0.7142\n",
      "Epoch 281/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5887 - acc: 0.7795 - val_loss: 0.8187 - val_acc: 0.7150\n",
      "Epoch 282/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5955 - acc: 0.7777 - val_loss: 0.8189 - val_acc: 0.7133\n",
      "Epoch 283/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5921 - acc: 0.7750 - val_loss: 0.8193 - val_acc: 0.7125\n",
      "Epoch 284/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5905 - acc: 0.7717 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 285/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5906 - acc: 0.7777 - val_loss: 0.8196 - val_acc: 0.7125\n",
      "Epoch 286/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5876 - acc: 0.7800 - val_loss: 0.8193 - val_acc: 0.7125\n",
      "Epoch 287/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5923 - acc: 0.7698 - val_loss: 0.8190 - val_acc: 0.7142\n",
      "Epoch 288/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5925 - acc: 0.7722 - val_loss: 0.8188 - val_acc: 0.7117\n",
      "Epoch 289/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5913 - acc: 0.7732 - val_loss: 0.8186 - val_acc: 0.7133\n",
      "Epoch 290/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5883 - acc: 0.7792 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 291/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5913 - acc: 0.7740 - val_loss: 0.8191 - val_acc: 0.7133\n",
      "Epoch 292/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5915 - acc: 0.7760 - val_loss: 0.8192 - val_acc: 0.7125\n",
      "Epoch 293/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5915 - acc: 0.7747 - val_loss: 0.8195 - val_acc: 0.7142\n",
      "Epoch 294/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5905 - acc: 0.7732 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 295/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5876 - acc: 0.7730 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5957 - acc: 0.7740 - val_loss: 0.8196 - val_acc: 0.7142\n",
      "Epoch 297/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5888 - acc: 0.7785 - val_loss: 0.8196 - val_acc: 0.7150\n",
      "Epoch 298/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5881 - acc: 0.7770 - val_loss: 0.8194 - val_acc: 0.7142\n",
      "Epoch 299/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5861 - acc: 0.7812 - val_loss: 0.8194 - val_acc: 0.7108\n",
      "Epoch 300/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5973 - acc: 0.7758 - val_loss: 0.8187 - val_acc: 0.7125\n",
      "Epoch 301/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5876 - acc: 0.7760 - val_loss: 0.8195 - val_acc: 0.7117\n",
      "Epoch 302/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5897 - acc: 0.7760 - val_loss: 0.8202 - val_acc: 0.7100\n",
      "Epoch 303/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5979 - acc: 0.7725 - val_loss: 0.8196 - val_acc: 0.7125\n",
      "Epoch 304/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5906 - acc: 0.7782 - val_loss: 0.8194 - val_acc: 0.7125\n",
      "Epoch 305/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5903 - acc: 0.7780 - val_loss: 0.8191 - val_acc: 0.7150\n",
      "Epoch 306/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5897 - acc: 0.7712 - val_loss: 0.8193 - val_acc: 0.7158\n",
      "Epoch 307/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5894 - acc: 0.7715 - val_loss: 0.8181 - val_acc: 0.7142\n",
      "Epoch 308/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5945 - acc: 0.7685 - val_loss: 0.8199 - val_acc: 0.7100\n",
      "Epoch 309/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5944 - acc: 0.7722 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 310/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5988 - acc: 0.7730 - val_loss: 0.8200 - val_acc: 0.7133\n",
      "Epoch 311/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5921 - acc: 0.7752 - val_loss: 0.8203 - val_acc: 0.7108\n",
      "Epoch 312/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5941 - acc: 0.7772 - val_loss: 0.8191 - val_acc: 0.7117\n",
      "Epoch 313/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5922 - acc: 0.7740 - val_loss: 0.8188 - val_acc: 0.7150\n",
      "Epoch 314/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5822 - acc: 0.7798 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 315/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5859 - acc: 0.7805 - val_loss: 0.8195 - val_acc: 0.7133\n",
      "Epoch 316/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5890 - acc: 0.7770 - val_loss: 0.8196 - val_acc: 0.7133\n",
      "Epoch 317/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5904 - acc: 0.7750 - val_loss: 0.8204 - val_acc: 0.7133\n",
      "Epoch 318/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5871 - acc: 0.7745 - val_loss: 0.8193 - val_acc: 0.7125\n",
      "Epoch 319/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5892 - acc: 0.7762 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 320/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5948 - acc: 0.7747 - val_loss: 0.8193 - val_acc: 0.7133\n",
      "Epoch 321/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5940 - acc: 0.7698 - val_loss: 0.8193 - val_acc: 0.7125\n",
      "Epoch 322/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5927 - acc: 0.7668 - val_loss: 0.8194 - val_acc: 0.7133\n",
      "Epoch 323/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5918 - acc: 0.7752 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 324/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5907 - acc: 0.7772 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 325/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5914 - acc: 0.7712 - val_loss: 0.8190 - val_acc: 0.7150\n",
      "Epoch 326/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5857 - acc: 0.7780 - val_loss: 0.8193 - val_acc: 0.7158\n",
      "Epoch 327/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5907 - acc: 0.7777 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 328/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5911 - acc: 0.7735 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 329/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5913 - acc: 0.7758 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 330/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5888 - acc: 0.7655 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 331/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5956 - acc: 0.7730 - val_loss: 0.8198 - val_acc: 0.7175\n",
      "Epoch 332/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5944 - acc: 0.7732 - val_loss: 0.8196 - val_acc: 0.7133\n",
      "Epoch 333/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5921 - acc: 0.7728 - val_loss: 0.8191 - val_acc: 0.7133\n",
      "Epoch 334/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5885 - acc: 0.7762 - val_loss: 0.8194 - val_acc: 0.7117\n",
      "Epoch 335/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5880 - acc: 0.7742 - val_loss: 0.8185 - val_acc: 0.7125\n",
      "Epoch 336/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5902 - acc: 0.7732 - val_loss: 0.8204 - val_acc: 0.7125\n",
      "Epoch 337/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5916 - acc: 0.7715 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 338/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5885 - acc: 0.7785 - val_loss: 0.8200 - val_acc: 0.7150\n",
      "Epoch 339/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5910 - acc: 0.7750 - val_loss: 0.8204 - val_acc: 0.7125\n",
      "Epoch 340/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5927 - acc: 0.7768 - val_loss: 0.8201 - val_acc: 0.7133\n",
      "Epoch 341/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5923 - acc: 0.7735 - val_loss: 0.8194 - val_acc: 0.7100\n",
      "Epoch 342/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5915 - acc: 0.7770 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 343/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5914 - acc: 0.7758 - val_loss: 0.8200 - val_acc: 0.7125\n",
      "Epoch 344/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5927 - acc: 0.7728 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 345/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5860 - acc: 0.7752 - val_loss: 0.8188 - val_acc: 0.7158\n",
      "Epoch 346/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5930 - acc: 0.7775 - val_loss: 0.8195 - val_acc: 0.7133\n",
      "Epoch 347/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5883 - acc: 0.7742 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 348/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5899 - acc: 0.7758 - val_loss: 0.8181 - val_acc: 0.7158\n",
      "Epoch 349/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5872 - acc: 0.7755 - val_loss: 0.8189 - val_acc: 0.7150\n",
      "Epoch 350/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5916 - acc: 0.7742 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 351/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5918 - acc: 0.7777 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 352/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5957 - acc: 0.7715 - val_loss: 0.8186 - val_acc: 0.7142\n",
      "Epoch 353/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5899 - acc: 0.7747 - val_loss: 0.8195 - val_acc: 0.7133\n",
      "Epoch 354/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5879 - acc: 0.7770 - val_loss: 0.8188 - val_acc: 0.7142\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5867 - acc: 0.7722 - val_loss: 0.8194 - val_acc: 0.7150\n",
      "Epoch 356/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5915 - acc: 0.7780 - val_loss: 0.8190 - val_acc: 0.7150\n",
      "Epoch 357/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5961 - acc: 0.7758 - val_loss: 0.8194 - val_acc: 0.7133\n",
      "Epoch 358/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5954 - acc: 0.7720 - val_loss: 0.8195 - val_acc: 0.7133\n",
      "Epoch 359/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5846 - acc: 0.7788 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 360/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5930 - acc: 0.7712 - val_loss: 0.8192 - val_acc: 0.7133\n",
      "Epoch 361/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5911 - acc: 0.7750 - val_loss: 0.8196 - val_acc: 0.7142\n",
      "Epoch 362/1000\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.5958 - acc: 0.7760 - val_loss: 0.8190 - val_acc: 0.7142\n",
      "Epoch 363/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5932 - acc: 0.7777 - val_loss: 0.8198 - val_acc: 0.7125\n",
      "Epoch 364/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5868 - acc: 0.7788 - val_loss: 0.8191 - val_acc: 0.7133\n",
      "Epoch 365/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5865 - acc: 0.7730 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 366/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5861 - acc: 0.7785 - val_loss: 0.8193 - val_acc: 0.7125\n",
      "Epoch 367/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5858 - acc: 0.7752 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 368/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5890 - acc: 0.7750 - val_loss: 0.8196 - val_acc: 0.7142\n",
      "Epoch 369/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5923 - acc: 0.7768 - val_loss: 0.8193 - val_acc: 0.7125\n",
      "Epoch 370/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5913 - acc: 0.7760 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 371/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5898 - acc: 0.7815 - val_loss: 0.8185 - val_acc: 0.7150\n",
      "Epoch 372/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5907 - acc: 0.7740 - val_loss: 0.8191 - val_acc: 0.7133\n",
      "Epoch 373/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5917 - acc: 0.7695 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 374/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5884 - acc: 0.7765 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 375/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5915 - acc: 0.7710 - val_loss: 0.8180 - val_acc: 0.7158\n",
      "Epoch 376/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5972 - acc: 0.7710 - val_loss: 0.8196 - val_acc: 0.7142\n",
      "Epoch 377/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5923 - acc: 0.7680 - val_loss: 0.8190 - val_acc: 0.7158\n",
      "Epoch 378/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5887 - acc: 0.7768 - val_loss: 0.8189 - val_acc: 0.7158\n",
      "Epoch 379/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5927 - acc: 0.7720 - val_loss: 0.8193 - val_acc: 0.7133\n",
      "Epoch 380/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5872 - acc: 0.7692 - val_loss: 0.8197 - val_acc: 0.7125\n",
      "Epoch 381/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5874 - acc: 0.7725 - val_loss: 0.8205 - val_acc: 0.7150\n",
      "Epoch 382/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5903 - acc: 0.7722 - val_loss: 0.8203 - val_acc: 0.7133\n",
      "Epoch 383/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5903 - acc: 0.7745 - val_loss: 0.8193 - val_acc: 0.7150\n",
      "Epoch 384/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5887 - acc: 0.7785 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 385/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5899 - acc: 0.7775 - val_loss: 0.8189 - val_acc: 0.7133\n",
      "Epoch 386/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5882 - acc: 0.7802 - val_loss: 0.8191 - val_acc: 0.7125\n",
      "Epoch 387/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5850 - acc: 0.7800 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 388/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5862 - acc: 0.7775 - val_loss: 0.8194 - val_acc: 0.7150\n",
      "Epoch 389/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5906 - acc: 0.7765 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 390/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5861 - acc: 0.7798 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 391/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5923 - acc: 0.7682 - val_loss: 0.8199 - val_acc: 0.7117\n",
      "Epoch 392/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5954 - acc: 0.7728 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 393/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5903 - acc: 0.7758 - val_loss: 0.8207 - val_acc: 0.7142\n",
      "Epoch 394/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5905 - acc: 0.7750 - val_loss: 0.8199 - val_acc: 0.7125\n",
      "Epoch 395/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5953 - acc: 0.7725 - val_loss: 0.8201 - val_acc: 0.7125\n",
      "Epoch 396/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5897 - acc: 0.7715 - val_loss: 0.8195 - val_acc: 0.7133\n",
      "Epoch 397/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5866 - acc: 0.7775 - val_loss: 0.8192 - val_acc: 0.7133\n",
      "Epoch 398/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5872 - acc: 0.7728 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 399/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5904 - acc: 0.7772 - val_loss: 0.8195 - val_acc: 0.7142\n",
      "Epoch 400/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5893 - acc: 0.7765 - val_loss: 0.8189 - val_acc: 0.7133\n",
      "Epoch 401/1000\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.5887 - acc: 0.7738 - val_loss: 0.8182 - val_acc: 0.7150\n",
      "Epoch 402/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5915 - acc: 0.7742 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 403/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5831 - acc: 0.7792 - val_loss: 0.8196 - val_acc: 0.7142\n",
      "Epoch 404/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5896 - acc: 0.7742 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 405/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5843 - acc: 0.7755 - val_loss: 0.8187 - val_acc: 0.7150\n",
      "Epoch 406/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5861 - acc: 0.7760 - val_loss: 0.8194 - val_acc: 0.7150\n",
      "Epoch 407/1000\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.5811 - acc: 0.7768 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 408/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5888 - acc: 0.7758 - val_loss: 0.8180 - val_acc: 0.7133\n",
      "Epoch 409/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5891 - acc: 0.7742 - val_loss: 0.8201 - val_acc: 0.7150\n",
      "Epoch 410/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5892 - acc: 0.7758 - val_loss: 0.8199 - val_acc: 0.7117\n",
      "Epoch 411/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5913 - acc: 0.7702 - val_loss: 0.8194 - val_acc: 0.7125\n",
      "Epoch 412/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5859 - acc: 0.7772 - val_loss: 0.8193 - val_acc: 0.7125\n",
      "Epoch 413/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5896 - acc: 0.7732 - val_loss: 0.8191 - val_acc: 0.7117\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5913 - acc: 0.7760 - val_loss: 0.8186 - val_acc: 0.7108\n",
      "Epoch 415/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5916 - acc: 0.7715 - val_loss: 0.8188 - val_acc: 0.7150\n",
      "Epoch 416/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5932 - acc: 0.7758 - val_loss: 0.8203 - val_acc: 0.7150\n",
      "Epoch 417/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5863 - acc: 0.7772 - val_loss: 0.8189 - val_acc: 0.7133\n",
      "Epoch 418/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5910 - acc: 0.7728 - val_loss: 0.8189 - val_acc: 0.7158\n",
      "Epoch 419/1000\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.5884 - acc: 0.7772 - val_loss: 0.8192 - val_acc: 0.7133\n",
      "Epoch 420/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5860 - acc: 0.7775 - val_loss: 0.8191 - val_acc: 0.7133\n",
      "Epoch 421/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5866 - acc: 0.7790 - val_loss: 0.8188 - val_acc: 0.7125\n",
      "Epoch 422/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5859 - acc: 0.7800 - val_loss: 0.8193 - val_acc: 0.7125\n",
      "Epoch 423/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5894 - acc: 0.7750 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 424/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5887 - acc: 0.7722 - val_loss: 0.8194 - val_acc: 0.7142\n",
      "Epoch 425/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5896 - acc: 0.7792 - val_loss: 0.8187 - val_acc: 0.7133\n",
      "Epoch 426/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5900 - acc: 0.7750 - val_loss: 0.8183 - val_acc: 0.7133\n",
      "Epoch 427/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5896 - acc: 0.7802 - val_loss: 0.8183 - val_acc: 0.7142\n",
      "Epoch 428/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5838 - acc: 0.7735 - val_loss: 0.8191 - val_acc: 0.7133\n",
      "Epoch 429/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5889 - acc: 0.7745 - val_loss: 0.8180 - val_acc: 0.7133\n",
      "Epoch 430/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5891 - acc: 0.7765 - val_loss: 0.8182 - val_acc: 0.7142\n",
      "Epoch 431/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5856 - acc: 0.7712 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 432/1000\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.5905 - acc: 0.7747 - val_loss: 0.8187 - val_acc: 0.7133\n",
      "Epoch 433/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5964 - acc: 0.7740 - val_loss: 0.8197 - val_acc: 0.7117\n",
      "Epoch 434/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5931 - acc: 0.7717 - val_loss: 0.8190 - val_acc: 0.7125\n",
      "Epoch 435/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5875 - acc: 0.7728 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 436/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5881 - acc: 0.7777 - val_loss: 0.8180 - val_acc: 0.7150\n",
      "Epoch 437/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5917 - acc: 0.7715 - val_loss: 0.8198 - val_acc: 0.7158\n",
      "Epoch 438/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5865 - acc: 0.7730 - val_loss: 0.8195 - val_acc: 0.7150\n",
      "Epoch 439/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5886 - acc: 0.7750 - val_loss: 0.8191 - val_acc: 0.7142\n",
      "Epoch 440/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5890 - acc: 0.7728 - val_loss: 0.8186 - val_acc: 0.7125\n",
      "Epoch 441/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5824 - acc: 0.7742 - val_loss: 0.8186 - val_acc: 0.7150\n",
      "Epoch 442/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5839 - acc: 0.7823 - val_loss: 0.8181 - val_acc: 0.7158\n",
      "Epoch 443/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5889 - acc: 0.7780 - val_loss: 0.8180 - val_acc: 0.7133\n",
      "Epoch 444/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5921 - acc: 0.7730 - val_loss: 0.8180 - val_acc: 0.7158\n",
      "Epoch 445/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5857 - acc: 0.7810 - val_loss: 0.8189 - val_acc: 0.7158\n",
      "Epoch 446/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5858 - acc: 0.7782 - val_loss: 0.8196 - val_acc: 0.7133\n",
      "Epoch 447/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5881 - acc: 0.7770 - val_loss: 0.8196 - val_acc: 0.7133\n",
      "Epoch 448/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5884 - acc: 0.7798 - val_loss: 0.8179 - val_acc: 0.7158\n",
      "Epoch 449/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5885 - acc: 0.7735 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 450/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5865 - acc: 0.7735 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 451/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5900 - acc: 0.7792 - val_loss: 0.8201 - val_acc: 0.7150\n",
      "Epoch 452/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5849 - acc: 0.7747 - val_loss: 0.8188 - val_acc: 0.7150\n",
      "Epoch 453/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5968 - acc: 0.7670 - val_loss: 0.8191 - val_acc: 0.7142\n",
      "Epoch 454/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5855 - acc: 0.7740 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 455/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5865 - acc: 0.7755 - val_loss: 0.8196 - val_acc: 0.7142\n",
      "Epoch 456/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5805 - acc: 0.7750 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 457/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5872 - acc: 0.7730 - val_loss: 0.8193 - val_acc: 0.7133\n",
      "Epoch 458/1000\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.5838 - acc: 0.7795 - val_loss: 0.8192 - val_acc: 0.7133\n",
      "Epoch 459/1000\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.5915 - acc: 0.7760 - val_loss: 0.8192 - val_acc: 0.7133\n",
      "Epoch 460/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5837 - acc: 0.7780 - val_loss: 0.8191 - val_acc: 0.7133\n",
      "Epoch 461/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5882 - acc: 0.7720 - val_loss: 0.8196 - val_acc: 0.7142\n",
      "Epoch 462/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5842 - acc: 0.7810 - val_loss: 0.8198 - val_acc: 0.7133\n",
      "Epoch 463/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5933 - acc: 0.7700 - val_loss: 0.8196 - val_acc: 0.7142\n",
      "Epoch 464/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5855 - acc: 0.7710 - val_loss: 0.8204 - val_acc: 0.7167\n",
      "Epoch 465/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5821 - acc: 0.7790 - val_loss: 0.8185 - val_acc: 0.7150\n",
      "Epoch 466/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5797 - acc: 0.7747 - val_loss: 0.8199 - val_acc: 0.7158\n",
      "Epoch 467/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5953 - acc: 0.7730 - val_loss: 0.8192 - val_acc: 0.7133\n",
      "Epoch 468/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5834 - acc: 0.7810 - val_loss: 0.8188 - val_acc: 0.7142\n",
      "Epoch 469/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5868 - acc: 0.7742 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 470/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5886 - acc: 0.7768 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 471/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5927 - acc: 0.7747 - val_loss: 0.8188 - val_acc: 0.7142\n",
      "Epoch 472/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5955 - acc: 0.7730 - val_loss: 0.8197 - val_acc: 0.7133\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5923 - acc: 0.7772 - val_loss: 0.8185 - val_acc: 0.7133\n",
      "Epoch 474/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5894 - acc: 0.7682 - val_loss: 0.8186 - val_acc: 0.7167\n",
      "Epoch 475/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5914 - acc: 0.7775 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 476/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5896 - acc: 0.7725 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 477/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5863 - acc: 0.7798 - val_loss: 0.8198 - val_acc: 0.7167\n",
      "Epoch 478/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5908 - acc: 0.7715 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 479/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5886 - acc: 0.7785 - val_loss: 0.8188 - val_acc: 0.7150\n",
      "Epoch 480/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5900 - acc: 0.7755 - val_loss: 0.8186 - val_acc: 0.7142\n",
      "Epoch 481/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5994 - acc: 0.7768 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 482/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5865 - acc: 0.7785 - val_loss: 0.8190 - val_acc: 0.7158\n",
      "Epoch 483/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5880 - acc: 0.7690 - val_loss: 0.8185 - val_acc: 0.7133\n",
      "Epoch 484/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5855 - acc: 0.7807 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 485/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5865 - acc: 0.7752 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 486/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5923 - acc: 0.7758 - val_loss: 0.8193 - val_acc: 0.7133\n",
      "Epoch 487/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5875 - acc: 0.7777 - val_loss: 0.8185 - val_acc: 0.7158\n",
      "Epoch 488/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5900 - acc: 0.7750 - val_loss: 0.8192 - val_acc: 0.7150\n",
      "Epoch 489/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5923 - acc: 0.7700 - val_loss: 0.8190 - val_acc: 0.7125\n",
      "Epoch 490/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5828 - acc: 0.7782 - val_loss: 0.8194 - val_acc: 0.7158\n",
      "Epoch 491/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5896 - acc: 0.7750 - val_loss: 0.8191 - val_acc: 0.7142\n",
      "Epoch 492/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5851 - acc: 0.7790 - val_loss: 0.8189 - val_acc: 0.7158\n",
      "Epoch 493/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5890 - acc: 0.7790 - val_loss: 0.8188 - val_acc: 0.7142\n",
      "Epoch 494/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5869 - acc: 0.7775 - val_loss: 0.8191 - val_acc: 0.7125\n",
      "Epoch 495/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5846 - acc: 0.7760 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 496/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5849 - acc: 0.7785 - val_loss: 0.8187 - val_acc: 0.7150\n",
      "Epoch 497/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5862 - acc: 0.7755 - val_loss: 0.8186 - val_acc: 0.7150\n",
      "Epoch 498/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5895 - acc: 0.7755 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 499/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5843 - acc: 0.7800 - val_loss: 0.8176 - val_acc: 0.7133\n",
      "Epoch 500/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5869 - acc: 0.7728 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 501/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5820 - acc: 0.7837 - val_loss: 0.8184 - val_acc: 0.7158\n",
      "Epoch 502/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5886 - acc: 0.7785 - val_loss: 0.8188 - val_acc: 0.7117\n",
      "Epoch 503/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5855 - acc: 0.7770 - val_loss: 0.8183 - val_acc: 0.7150\n",
      "Epoch 504/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5888 - acc: 0.7708 - val_loss: 0.8185 - val_acc: 0.7133\n",
      "Epoch 505/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5918 - acc: 0.7732 - val_loss: 0.8191 - val_acc: 0.7125\n",
      "Epoch 506/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5861 - acc: 0.7770 - val_loss: 0.8179 - val_acc: 0.7142\n",
      "Epoch 507/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5849 - acc: 0.7800 - val_loss: 0.8180 - val_acc: 0.7150\n",
      "Epoch 508/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5842 - acc: 0.7788 - val_loss: 0.8187 - val_acc: 0.7133\n",
      "Epoch 509/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5879 - acc: 0.7745 - val_loss: 0.8176 - val_acc: 0.7142\n",
      "Epoch 510/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5881 - acc: 0.7762 - val_loss: 0.8176 - val_acc: 0.7150\n",
      "Epoch 511/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5911 - acc: 0.7745 - val_loss: 0.8179 - val_acc: 0.7150\n",
      "Epoch 512/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5866 - acc: 0.7725 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 513/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5848 - acc: 0.7823 - val_loss: 0.8177 - val_acc: 0.7133\n",
      "Epoch 514/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5879 - acc: 0.7845 - val_loss: 0.8186 - val_acc: 0.7108\n",
      "Epoch 515/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5911 - acc: 0.7768 - val_loss: 0.8181 - val_acc: 0.7158\n",
      "Epoch 516/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5865 - acc: 0.7772 - val_loss: 0.8177 - val_acc: 0.7175\n",
      "Epoch 517/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5874 - acc: 0.7825 - val_loss: 0.8174 - val_acc: 0.7142\n",
      "Epoch 518/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5871 - acc: 0.7680 - val_loss: 0.8176 - val_acc: 0.7167\n",
      "Epoch 519/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5866 - acc: 0.7765 - val_loss: 0.8193 - val_acc: 0.7142\n",
      "Epoch 520/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5829 - acc: 0.7798 - val_loss: 0.8188 - val_acc: 0.7125\n",
      "Epoch 521/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5870 - acc: 0.7720 - val_loss: 0.8192 - val_acc: 0.7150\n",
      "Epoch 522/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5849 - acc: 0.7752 - val_loss: 0.8173 - val_acc: 0.7100\n",
      "Epoch 523/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5862 - acc: 0.7780 - val_loss: 0.8183 - val_acc: 0.7150\n",
      "Epoch 524/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5882 - acc: 0.7768 - val_loss: 0.8184 - val_acc: 0.7125\n",
      "Epoch 525/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5821 - acc: 0.7777 - val_loss: 0.8187 - val_acc: 0.7133\n",
      "Epoch 526/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5839 - acc: 0.7810 - val_loss: 0.8183 - val_acc: 0.7150\n",
      "Epoch 527/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5815 - acc: 0.7782 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 528/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5873 - acc: 0.7750 - val_loss: 0.8176 - val_acc: 0.7158\n",
      "Epoch 529/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5902 - acc: 0.7752 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 530/1000\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.5843 - acc: 0.7807 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 531/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5866 - acc: 0.7747 - val_loss: 0.8195 - val_acc: 0.7142\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5898 - acc: 0.7795 - val_loss: 0.8182 - val_acc: 0.7142\n",
      "Epoch 533/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5890 - acc: 0.7777 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 534/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5913 - acc: 0.7730 - val_loss: 0.8181 - val_acc: 0.7142\n",
      "Epoch 535/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5879 - acc: 0.7730 - val_loss: 0.8190 - val_acc: 0.7150\n",
      "Epoch 536/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5843 - acc: 0.7788 - val_loss: 0.8183 - val_acc: 0.7133\n",
      "Epoch 537/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5842 - acc: 0.7785 - val_loss: 0.8178 - val_acc: 0.7150\n",
      "Epoch 538/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5913 - acc: 0.7755 - val_loss: 0.8181 - val_acc: 0.7133\n",
      "Epoch 539/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5842 - acc: 0.7812 - val_loss: 0.8171 - val_acc: 0.7183\n",
      "Epoch 540/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5845 - acc: 0.7795 - val_loss: 0.8185 - val_acc: 0.7133\n",
      "Epoch 541/1000\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.5874 - acc: 0.7772 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 542/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5898 - acc: 0.7728 - val_loss: 0.8184 - val_acc: 0.7142\n",
      "Epoch 543/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5898 - acc: 0.7740 - val_loss: 0.8182 - val_acc: 0.7133\n",
      "Epoch 544/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5868 - acc: 0.7755 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 545/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5919 - acc: 0.7805 - val_loss: 0.8187 - val_acc: 0.7167\n",
      "Epoch 546/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5822 - acc: 0.7730 - val_loss: 0.8185 - val_acc: 0.7125\n",
      "Epoch 547/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5856 - acc: 0.7715 - val_loss: 0.8183 - val_acc: 0.7150\n",
      "Epoch 548/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5860 - acc: 0.7790 - val_loss: 0.8174 - val_acc: 0.7125\n",
      "Epoch 549/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5843 - acc: 0.7782 - val_loss: 0.8193 - val_acc: 0.7150\n",
      "Epoch 550/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5838 - acc: 0.7762 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 551/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5819 - acc: 0.7740 - val_loss: 0.8179 - val_acc: 0.7158\n",
      "Epoch 552/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5885 - acc: 0.7755 - val_loss: 0.8186 - val_acc: 0.7142\n",
      "Epoch 553/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5910 - acc: 0.7730 - val_loss: 0.8187 - val_acc: 0.7125\n",
      "Epoch 554/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5876 - acc: 0.7715 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 555/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5841 - acc: 0.7775 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 556/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5918 - acc: 0.7698 - val_loss: 0.8182 - val_acc: 0.7150\n",
      "Epoch 557/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5903 - acc: 0.7732 - val_loss: 0.8184 - val_acc: 0.7125\n",
      "Epoch 558/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5861 - acc: 0.7710 - val_loss: 0.8187 - val_acc: 0.7125\n",
      "Epoch 559/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5855 - acc: 0.7730 - val_loss: 0.8183 - val_acc: 0.7158\n",
      "Epoch 560/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5819 - acc: 0.7830 - val_loss: 0.8184 - val_acc: 0.7158\n",
      "Epoch 561/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5874 - acc: 0.7780 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 562/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5831 - acc: 0.7775 - val_loss: 0.8183 - val_acc: 0.7158\n",
      "Epoch 563/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5791 - acc: 0.7807 - val_loss: 0.8180 - val_acc: 0.7167\n",
      "Epoch 564/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5839 - acc: 0.7810 - val_loss: 0.8188 - val_acc: 0.7167\n",
      "Epoch 565/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5901 - acc: 0.7670 - val_loss: 0.8176 - val_acc: 0.7158\n",
      "Epoch 566/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5875 - acc: 0.7770 - val_loss: 0.8185 - val_acc: 0.7125\n",
      "Epoch 567/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5904 - acc: 0.7728 - val_loss: 0.8195 - val_acc: 0.7142\n",
      "Epoch 568/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5835 - acc: 0.7810 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 569/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5837 - acc: 0.7755 - val_loss: 0.8189 - val_acc: 0.7150\n",
      "Epoch 570/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5825 - acc: 0.7735 - val_loss: 0.8192 - val_acc: 0.7142\n",
      "Epoch 571/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5896 - acc: 0.7775 - val_loss: 0.8193 - val_acc: 0.7150\n",
      "Epoch 572/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5802 - acc: 0.7807 - val_loss: 0.8188 - val_acc: 0.7142\n",
      "Epoch 573/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5839 - acc: 0.7772 - val_loss: 0.8188 - val_acc: 0.7133\n",
      "Epoch 574/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5887 - acc: 0.7758 - val_loss: 0.8193 - val_acc: 0.7150\n",
      "Epoch 575/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5869 - acc: 0.7788 - val_loss: 0.8194 - val_acc: 0.7150\n",
      "Epoch 576/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5914 - acc: 0.7738 - val_loss: 0.8177 - val_acc: 0.7150\n",
      "Epoch 577/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5784 - acc: 0.7750 - val_loss: 0.8193 - val_acc: 0.7150\n",
      "Epoch 578/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5840 - acc: 0.7785 - val_loss: 0.8182 - val_acc: 0.7167\n",
      "Epoch 579/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5884 - acc: 0.7800 - val_loss: 0.8189 - val_acc: 0.7150\n",
      "Epoch 580/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5890 - acc: 0.7752 - val_loss: 0.8187 - val_acc: 0.7133\n",
      "Epoch 581/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5871 - acc: 0.7747 - val_loss: 0.8186 - val_acc: 0.7150\n",
      "Epoch 582/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5851 - acc: 0.7823 - val_loss: 0.8181 - val_acc: 0.7150\n",
      "Epoch 583/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5843 - acc: 0.7777 - val_loss: 0.8188 - val_acc: 0.7150\n",
      "Epoch 584/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5829 - acc: 0.7752 - val_loss: 0.8192 - val_acc: 0.7150\n",
      "Epoch 585/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5884 - acc: 0.7752 - val_loss: 0.8191 - val_acc: 0.7150\n",
      "Epoch 586/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5768 - acc: 0.7855 - val_loss: 0.8184 - val_acc: 0.7167\n",
      "Epoch 587/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5868 - acc: 0.7712 - val_loss: 0.8199 - val_acc: 0.7150\n",
      "Epoch 588/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5791 - acc: 0.7790 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 589/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5870 - acc: 0.7765 - val_loss: 0.8181 - val_acc: 0.7158\n",
      "Epoch 590/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5824 - acc: 0.7775 - val_loss: 0.8193 - val_acc: 0.7108\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5864 - acc: 0.7782 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 592/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5820 - acc: 0.7805 - val_loss: 0.8184 - val_acc: 0.7142\n",
      "Epoch 593/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5835 - acc: 0.7755 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 594/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5808 - acc: 0.7730 - val_loss: 0.8186 - val_acc: 0.7150\n",
      "Epoch 595/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5868 - acc: 0.7725 - val_loss: 0.8199 - val_acc: 0.7133\n",
      "Epoch 596/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5900 - acc: 0.7760 - val_loss: 0.8180 - val_acc: 0.7167\n",
      "Epoch 597/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5834 - acc: 0.7732 - val_loss: 0.8181 - val_acc: 0.7125\n",
      "Epoch 598/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5817 - acc: 0.7785 - val_loss: 0.8183 - val_acc: 0.7133\n",
      "Epoch 599/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5806 - acc: 0.7777 - val_loss: 0.8188 - val_acc: 0.7125\n",
      "Epoch 600/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5842 - acc: 0.7750 - val_loss: 0.8180 - val_acc: 0.7142\n",
      "Epoch 601/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5901 - acc: 0.7725 - val_loss: 0.8180 - val_acc: 0.7142\n",
      "Epoch 602/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5815 - acc: 0.7772 - val_loss: 0.8172 - val_acc: 0.7142\n",
      "Epoch 603/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5823 - acc: 0.7825 - val_loss: 0.8172 - val_acc: 0.7158\n",
      "Epoch 604/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5875 - acc: 0.7738 - val_loss: 0.8181 - val_acc: 0.7142\n",
      "Epoch 605/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5868 - acc: 0.7730 - val_loss: 0.8179 - val_acc: 0.7150\n",
      "Epoch 606/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5829 - acc: 0.7792 - val_loss: 0.8174 - val_acc: 0.7167\n",
      "Epoch 607/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5800 - acc: 0.7820 - val_loss: 0.8191 - val_acc: 0.7125\n",
      "Epoch 608/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5829 - acc: 0.7782 - val_loss: 0.8199 - val_acc: 0.7158\n",
      "Epoch 609/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5804 - acc: 0.7758 - val_loss: 0.8179 - val_acc: 0.7158\n",
      "Epoch 610/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5869 - acc: 0.7812 - val_loss: 0.8181 - val_acc: 0.7150\n",
      "Epoch 611/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5841 - acc: 0.7725 - val_loss: 0.8185 - val_acc: 0.7133\n",
      "Epoch 612/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5877 - acc: 0.7738 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 613/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5841 - acc: 0.7747 - val_loss: 0.8185 - val_acc: 0.7125\n",
      "Epoch 614/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5837 - acc: 0.7768 - val_loss: 0.8180 - val_acc: 0.7150\n",
      "Epoch 615/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5872 - acc: 0.7795 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 616/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5828 - acc: 0.7738 - val_loss: 0.8187 - val_acc: 0.7125\n",
      "Epoch 617/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5876 - acc: 0.7768 - val_loss: 0.8180 - val_acc: 0.7142\n",
      "Epoch 618/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5885 - acc: 0.7790 - val_loss: 0.8184 - val_acc: 0.7125\n",
      "Epoch 619/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5828 - acc: 0.7805 - val_loss: 0.8181 - val_acc: 0.7125\n",
      "Epoch 620/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5888 - acc: 0.7812 - val_loss: 0.8175 - val_acc: 0.7158\n",
      "Epoch 621/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5834 - acc: 0.7798 - val_loss: 0.8176 - val_acc: 0.7158\n",
      "Epoch 622/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5834 - acc: 0.7728 - val_loss: 0.8171 - val_acc: 0.7158\n",
      "Epoch 623/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5816 - acc: 0.7768 - val_loss: 0.8176 - val_acc: 0.7150\n",
      "Epoch 624/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5865 - acc: 0.7768 - val_loss: 0.8178 - val_acc: 0.7150\n",
      "Epoch 625/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5797 - acc: 0.7788 - val_loss: 0.8188 - val_acc: 0.7142\n",
      "Epoch 626/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5839 - acc: 0.7810 - val_loss: 0.8194 - val_acc: 0.7150\n",
      "Epoch 627/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5829 - acc: 0.7798 - val_loss: 0.8192 - val_acc: 0.7150\n",
      "Epoch 628/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5841 - acc: 0.7758 - val_loss: 0.8174 - val_acc: 0.7150\n",
      "Epoch 629/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5845 - acc: 0.7798 - val_loss: 0.8184 - val_acc: 0.7125\n",
      "Epoch 630/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5827 - acc: 0.7805 - val_loss: 0.8179 - val_acc: 0.7133\n",
      "Epoch 631/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5897 - acc: 0.7735 - val_loss: 0.8181 - val_acc: 0.7108\n",
      "Epoch 632/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5791 - acc: 0.7805 - val_loss: 0.8173 - val_acc: 0.7158\n",
      "Epoch 633/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5841 - acc: 0.7792 - val_loss: 0.8181 - val_acc: 0.7142\n",
      "Epoch 634/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5855 - acc: 0.7788 - val_loss: 0.8180 - val_acc: 0.7150\n",
      "Epoch 635/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5805 - acc: 0.7790 - val_loss: 0.8181 - val_acc: 0.7125\n",
      "Epoch 636/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5896 - acc: 0.7732 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 637/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5843 - acc: 0.7760 - val_loss: 0.8186 - val_acc: 0.7150\n",
      "Epoch 638/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5844 - acc: 0.7782 - val_loss: 0.8185 - val_acc: 0.7133\n",
      "Epoch 639/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5811 - acc: 0.7765 - val_loss: 0.8184 - val_acc: 0.7125\n",
      "Epoch 640/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5850 - acc: 0.7825 - val_loss: 0.8186 - val_acc: 0.7133\n",
      "Epoch 641/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5833 - acc: 0.7708 - val_loss: 0.8180 - val_acc: 0.7150\n",
      "Epoch 642/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5868 - acc: 0.7798 - val_loss: 0.8188 - val_acc: 0.7150\n",
      "Epoch 643/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5788 - acc: 0.7738 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 644/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5875 - acc: 0.7788 - val_loss: 0.8177 - val_acc: 0.7142\n",
      "Epoch 645/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5860 - acc: 0.7752 - val_loss: 0.8189 - val_acc: 0.7133\n",
      "Epoch 646/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5844 - acc: 0.7790 - val_loss: 0.8186 - val_acc: 0.7125\n",
      "Epoch 647/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5807 - acc: 0.7768 - val_loss: 0.8169 - val_acc: 0.7150\n",
      "Epoch 648/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5819 - acc: 0.7695 - val_loss: 0.8177 - val_acc: 0.7150\n",
      "Epoch 649/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5821 - acc: 0.7818 - val_loss: 0.8189 - val_acc: 0.7125\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5787 - acc: 0.7802 - val_loss: 0.8179 - val_acc: 0.7133\n",
      "Epoch 651/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5835 - acc: 0.7780 - val_loss: 0.8178 - val_acc: 0.7125\n",
      "Epoch 652/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5861 - acc: 0.7768 - val_loss: 0.8179 - val_acc: 0.7108\n",
      "Epoch 653/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5816 - acc: 0.7805 - val_loss: 0.8182 - val_acc: 0.7125\n",
      "Epoch 654/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5841 - acc: 0.7742 - val_loss: 0.8189 - val_acc: 0.7150\n",
      "Epoch 655/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5814 - acc: 0.7798 - val_loss: 0.8173 - val_acc: 0.7158\n",
      "Epoch 656/1000\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.5858 - acc: 0.7722 - val_loss: 0.8181 - val_acc: 0.7117\n",
      "Epoch 657/1000\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.5873 - acc: 0.7717 - val_loss: 0.8181 - val_acc: 0.7142\n",
      "Epoch 658/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5795 - acc: 0.7802 - val_loss: 0.8181 - val_acc: 0.7142\n",
      "Epoch 659/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5844 - acc: 0.7777 - val_loss: 0.8190 - val_acc: 0.7125\n",
      "Epoch 660/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5833 - acc: 0.7782 - val_loss: 0.8189 - val_acc: 0.7108\n",
      "Epoch 661/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5917 - acc: 0.7738 - val_loss: 0.8184 - val_acc: 0.7133\n",
      "Epoch 662/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5871 - acc: 0.7755 - val_loss: 0.8188 - val_acc: 0.7117\n",
      "Epoch 663/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5839 - acc: 0.7715 - val_loss: 0.8184 - val_acc: 0.7117\n",
      "Epoch 664/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5839 - acc: 0.7805 - val_loss: 0.8186 - val_acc: 0.7117\n",
      "Epoch 665/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5800 - acc: 0.7800 - val_loss: 0.8174 - val_acc: 0.7158\n",
      "Epoch 666/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5822 - acc: 0.7802 - val_loss: 0.8179 - val_acc: 0.7142\n",
      "Epoch 667/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5877 - acc: 0.7695 - val_loss: 0.8183 - val_acc: 0.7133\n",
      "Epoch 668/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5895 - acc: 0.7770 - val_loss: 0.8183 - val_acc: 0.7142\n",
      "Epoch 669/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5874 - acc: 0.7710 - val_loss: 0.8172 - val_acc: 0.7167\n",
      "Epoch 670/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5873 - acc: 0.7758 - val_loss: 0.8187 - val_acc: 0.7133\n",
      "Epoch 671/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5809 - acc: 0.7818 - val_loss: 0.8171 - val_acc: 0.7150\n",
      "Epoch 672/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5832 - acc: 0.7802 - val_loss: 0.8187 - val_acc: 0.7133\n",
      "Epoch 673/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5818 - acc: 0.7823 - val_loss: 0.8189 - val_acc: 0.7150\n",
      "Epoch 674/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5816 - acc: 0.7802 - val_loss: 0.8190 - val_acc: 0.7133\n",
      "Epoch 675/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5837 - acc: 0.7782 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 676/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5842 - acc: 0.7777 - val_loss: 0.8180 - val_acc: 0.7167\n",
      "Epoch 677/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5868 - acc: 0.7722 - val_loss: 0.8177 - val_acc: 0.7150\n",
      "Epoch 678/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5761 - acc: 0.7823 - val_loss: 0.8195 - val_acc: 0.7142\n",
      "Epoch 679/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5830 - acc: 0.7758 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 680/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5800 - acc: 0.7768 - val_loss: 0.8180 - val_acc: 0.7142\n",
      "Epoch 681/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5843 - acc: 0.7738 - val_loss: 0.8174 - val_acc: 0.7158\n",
      "Epoch 682/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5797 - acc: 0.7798 - val_loss: 0.8197 - val_acc: 0.7142\n",
      "Epoch 683/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5835 - acc: 0.7747 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 684/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5886 - acc: 0.7740 - val_loss: 0.8178 - val_acc: 0.7150\n",
      "Epoch 685/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5839 - acc: 0.7722 - val_loss: 0.8189 - val_acc: 0.7133\n",
      "Epoch 686/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5837 - acc: 0.7732 - val_loss: 0.8179 - val_acc: 0.7158\n",
      "Epoch 687/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5746 - acc: 0.7825 - val_loss: 0.8175 - val_acc: 0.7158\n",
      "Epoch 688/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5843 - acc: 0.7815 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 689/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5779 - acc: 0.7833 - val_loss: 0.8176 - val_acc: 0.7142\n",
      "Epoch 690/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5844 - acc: 0.7798 - val_loss: 0.8176 - val_acc: 0.7142\n",
      "Epoch 691/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5830 - acc: 0.7708 - val_loss: 0.8182 - val_acc: 0.7142\n",
      "Epoch 692/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5845 - acc: 0.7730 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 693/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5857 - acc: 0.7780 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 694/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5858 - acc: 0.7843 - val_loss: 0.8174 - val_acc: 0.7175\n",
      "Epoch 695/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5851 - acc: 0.7762 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 696/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5768 - acc: 0.7815 - val_loss: 0.8176 - val_acc: 0.7183\n",
      "Epoch 697/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5813 - acc: 0.7738 - val_loss: 0.8177 - val_acc: 0.7158\n",
      "Epoch 698/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5817 - acc: 0.7805 - val_loss: 0.8180 - val_acc: 0.7167\n",
      "Epoch 699/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5814 - acc: 0.7758 - val_loss: 0.8181 - val_acc: 0.7150\n",
      "Epoch 700/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5825 - acc: 0.7775 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 701/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5857 - acc: 0.7762 - val_loss: 0.8176 - val_acc: 0.7167\n",
      "Epoch 702/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5811 - acc: 0.7770 - val_loss: 0.8186 - val_acc: 0.7150\n",
      "Epoch 703/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5829 - acc: 0.7740 - val_loss: 0.8183 - val_acc: 0.7142\n",
      "Epoch 704/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5827 - acc: 0.7768 - val_loss: 0.8191 - val_acc: 0.7142\n",
      "Epoch 705/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5784 - acc: 0.7863 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 706/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5820 - acc: 0.7810 - val_loss: 0.8179 - val_acc: 0.7150\n",
      "Epoch 707/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5784 - acc: 0.7845 - val_loss: 0.8181 - val_acc: 0.7150\n",
      "Epoch 708/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5811 - acc: 0.7812 - val_loss: 0.8186 - val_acc: 0.7150\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5880 - acc: 0.7738 - val_loss: 0.8175 - val_acc: 0.7167\n",
      "Epoch 710/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5784 - acc: 0.7827 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 711/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5855 - acc: 0.7780 - val_loss: 0.8175 - val_acc: 0.7158\n",
      "Epoch 712/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5769 - acc: 0.7860 - val_loss: 0.8169 - val_acc: 0.7183\n",
      "Epoch 713/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5831 - acc: 0.7777 - val_loss: 0.8177 - val_acc: 0.7158\n",
      "Epoch 714/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5835 - acc: 0.7735 - val_loss: 0.8189 - val_acc: 0.7142\n",
      "Epoch 715/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5805 - acc: 0.7812 - val_loss: 0.8172 - val_acc: 0.7150\n",
      "Epoch 716/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5783 - acc: 0.7830 - val_loss: 0.8187 - val_acc: 0.7142\n",
      "Epoch 717/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5830 - acc: 0.7820 - val_loss: 0.8181 - val_acc: 0.7133\n",
      "Epoch 718/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5771 - acc: 0.7798 - val_loss: 0.8184 - val_acc: 0.7142\n",
      "Epoch 719/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5802 - acc: 0.7775 - val_loss: 0.8182 - val_acc: 0.7158\n",
      "Epoch 720/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5796 - acc: 0.7785 - val_loss: 0.8186 - val_acc: 0.7158\n",
      "Epoch 721/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5770 - acc: 0.7812 - val_loss: 0.8183 - val_acc: 0.7150\n",
      "Epoch 722/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5845 - acc: 0.7772 - val_loss: 0.8176 - val_acc: 0.7167\n",
      "Epoch 723/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5802 - acc: 0.7780 - val_loss: 0.8174 - val_acc: 0.7150\n",
      "Epoch 724/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5828 - acc: 0.7772 - val_loss: 0.8172 - val_acc: 0.7150\n",
      "Epoch 725/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5789 - acc: 0.7800 - val_loss: 0.8172 - val_acc: 0.7142\n",
      "Epoch 726/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5791 - acc: 0.7890 - val_loss: 0.8178 - val_acc: 0.7150\n",
      "Epoch 727/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5847 - acc: 0.7823 - val_loss: 0.8173 - val_acc: 0.7158\n",
      "Epoch 728/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5823 - acc: 0.7768 - val_loss: 0.8173 - val_acc: 0.7133\n",
      "Epoch 729/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5809 - acc: 0.7770 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 730/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5823 - acc: 0.7772 - val_loss: 0.8178 - val_acc: 0.7133\n",
      "Epoch 731/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5753 - acc: 0.7840 - val_loss: 0.8177 - val_acc: 0.7142\n",
      "Epoch 732/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5820 - acc: 0.7738 - val_loss: 0.8173 - val_acc: 0.7167\n",
      "Epoch 733/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5776 - acc: 0.7758 - val_loss: 0.8183 - val_acc: 0.7133\n",
      "Epoch 734/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5817 - acc: 0.7775 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 735/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5804 - acc: 0.7800 - val_loss: 0.8181 - val_acc: 0.7150\n",
      "Epoch 736/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5915 - acc: 0.7747 - val_loss: 0.8189 - val_acc: 0.7167\n",
      "Epoch 737/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5769 - acc: 0.7782 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 738/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5794 - acc: 0.7805 - val_loss: 0.8176 - val_acc: 0.7158\n",
      "Epoch 739/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5748 - acc: 0.7798 - val_loss: 0.8174 - val_acc: 0.7167\n",
      "Epoch 740/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5791 - acc: 0.7772 - val_loss: 0.8176 - val_acc: 0.7150\n",
      "Epoch 741/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5837 - acc: 0.7738 - val_loss: 0.8185 - val_acc: 0.7158\n",
      "Epoch 742/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5829 - acc: 0.7812 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 743/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5819 - acc: 0.7788 - val_loss: 0.8195 - val_acc: 0.7150\n",
      "Epoch 744/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5847 - acc: 0.7745 - val_loss: 0.8183 - val_acc: 0.7117\n",
      "Epoch 745/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5805 - acc: 0.7798 - val_loss: 0.8180 - val_acc: 0.7125\n",
      "Epoch 746/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5837 - acc: 0.7775 - val_loss: 0.8177 - val_acc: 0.7167\n",
      "Epoch 747/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5726 - acc: 0.7795 - val_loss: 0.8185 - val_acc: 0.7125\n",
      "Epoch 748/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5793 - acc: 0.7807 - val_loss: 0.8175 - val_acc: 0.7167\n",
      "Epoch 749/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5852 - acc: 0.7777 - val_loss: 0.8182 - val_acc: 0.7158\n",
      "Epoch 750/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5826 - acc: 0.7725 - val_loss: 0.8186 - val_acc: 0.7133\n",
      "Epoch 751/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5845 - acc: 0.7745 - val_loss: 0.8181 - val_acc: 0.7133\n",
      "Epoch 752/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5793 - acc: 0.7765 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 753/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5798 - acc: 0.7780 - val_loss: 0.8180 - val_acc: 0.7142\n",
      "Epoch 754/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5805 - acc: 0.7775 - val_loss: 0.8177 - val_acc: 0.7175\n",
      "Epoch 755/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5870 - acc: 0.7717 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 756/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5845 - acc: 0.7715 - val_loss: 0.8181 - val_acc: 0.7133\n",
      "Epoch 757/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5767 - acc: 0.7815 - val_loss: 0.8173 - val_acc: 0.7183\n",
      "Epoch 758/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5833 - acc: 0.7820 - val_loss: 0.8180 - val_acc: 0.7158\n",
      "Epoch 759/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5794 - acc: 0.7810 - val_loss: 0.8185 - val_acc: 0.7117\n",
      "Epoch 760/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5842 - acc: 0.7762 - val_loss: 0.8175 - val_acc: 0.7158\n",
      "Epoch 761/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5803 - acc: 0.7752 - val_loss: 0.8172 - val_acc: 0.7167\n",
      "Epoch 762/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5855 - acc: 0.7798 - val_loss: 0.8176 - val_acc: 0.7158\n",
      "Epoch 763/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5821 - acc: 0.7780 - val_loss: 0.8166 - val_acc: 0.7183\n",
      "Epoch 764/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5825 - acc: 0.7725 - val_loss: 0.8177 - val_acc: 0.7167\n",
      "Epoch 765/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5767 - acc: 0.7807 - val_loss: 0.8188 - val_acc: 0.7158\n",
      "Epoch 766/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5766 - acc: 0.7792 - val_loss: 0.8176 - val_acc: 0.7167\n",
      "Epoch 767/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5800 - acc: 0.7830 - val_loss: 0.8176 - val_acc: 0.7158\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5770 - acc: 0.7810 - val_loss: 0.8177 - val_acc: 0.7158\n",
      "Epoch 769/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5747 - acc: 0.7825 - val_loss: 0.8181 - val_acc: 0.7158\n",
      "Epoch 770/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5727 - acc: 0.7812 - val_loss: 0.8185 - val_acc: 0.7150\n",
      "Epoch 771/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5783 - acc: 0.7840 - val_loss: 0.8180 - val_acc: 0.7142\n",
      "Epoch 772/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5757 - acc: 0.7788 - val_loss: 0.8178 - val_acc: 0.7133\n",
      "Epoch 773/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5771 - acc: 0.7795 - val_loss: 0.8178 - val_acc: 0.7133\n",
      "Epoch 774/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5813 - acc: 0.7738 - val_loss: 0.8175 - val_acc: 0.7142\n",
      "Epoch 775/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5780 - acc: 0.7848 - val_loss: 0.8173 - val_acc: 0.7150\n",
      "Epoch 776/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5791 - acc: 0.7792 - val_loss: 0.8174 - val_acc: 0.7150\n",
      "Epoch 777/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5784 - acc: 0.7760 - val_loss: 0.8168 - val_acc: 0.7167\n",
      "Epoch 778/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5760 - acc: 0.7823 - val_loss: 0.8169 - val_acc: 0.7150\n",
      "Epoch 779/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5840 - acc: 0.7812 - val_loss: 0.8168 - val_acc: 0.7167\n",
      "Epoch 780/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5784 - acc: 0.7777 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 781/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5762 - acc: 0.7853 - val_loss: 0.8179 - val_acc: 0.7150\n",
      "Epoch 782/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5788 - acc: 0.7812 - val_loss: 0.8186 - val_acc: 0.7158\n",
      "Epoch 783/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5761 - acc: 0.7845 - val_loss: 0.8169 - val_acc: 0.7142\n",
      "Epoch 784/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5872 - acc: 0.7758 - val_loss: 0.8161 - val_acc: 0.7175\n",
      "Epoch 785/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5756 - acc: 0.7890 - val_loss: 0.8168 - val_acc: 0.7158\n",
      "Epoch 786/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5822 - acc: 0.7785 - val_loss: 0.8176 - val_acc: 0.7133\n",
      "Epoch 787/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5789 - acc: 0.7795 - val_loss: 0.8182 - val_acc: 0.7133\n",
      "Epoch 788/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5848 - acc: 0.7740 - val_loss: 0.8165 - val_acc: 0.7150\n",
      "Epoch 789/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5770 - acc: 0.7777 - val_loss: 0.8171 - val_acc: 0.7158\n",
      "Epoch 790/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5792 - acc: 0.7825 - val_loss: 0.8173 - val_acc: 0.7167\n",
      "Epoch 791/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5819 - acc: 0.7768 - val_loss: 0.8174 - val_acc: 0.7142\n",
      "Epoch 792/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5828 - acc: 0.7725 - val_loss: 0.8177 - val_acc: 0.7158\n",
      "Epoch 793/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5766 - acc: 0.7788 - val_loss: 0.8179 - val_acc: 0.7158\n",
      "Epoch 794/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5766 - acc: 0.7825 - val_loss: 0.8173 - val_acc: 0.7150\n",
      "Epoch 795/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5802 - acc: 0.7775 - val_loss: 0.8176 - val_acc: 0.7158\n",
      "Epoch 796/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5890 - acc: 0.7752 - val_loss: 0.8171 - val_acc: 0.7167\n",
      "Epoch 797/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5728 - acc: 0.7860 - val_loss: 0.8176 - val_acc: 0.7142\n",
      "Epoch 798/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5737 - acc: 0.7760 - val_loss: 0.8175 - val_acc: 0.7133\n",
      "Epoch 799/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5849 - acc: 0.7780 - val_loss: 0.8172 - val_acc: 0.7133\n",
      "Epoch 800/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5794 - acc: 0.7747 - val_loss: 0.8169 - val_acc: 0.7158\n",
      "Epoch 801/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5800 - acc: 0.7755 - val_loss: 0.8171 - val_acc: 0.7150\n",
      "Epoch 802/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5796 - acc: 0.7805 - val_loss: 0.8179 - val_acc: 0.7133\n",
      "Epoch 803/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5764 - acc: 0.7792 - val_loss: 0.8172 - val_acc: 0.7175\n",
      "Epoch 804/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5827 - acc: 0.7795 - val_loss: 0.8166 - val_acc: 0.7158\n",
      "Epoch 805/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5853 - acc: 0.7768 - val_loss: 0.8185 - val_acc: 0.7150\n",
      "Epoch 806/1000\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.5839 - acc: 0.7785 - val_loss: 0.8179 - val_acc: 0.7133\n",
      "Epoch 807/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5820 - acc: 0.7805 - val_loss: 0.8186 - val_acc: 0.7133\n",
      "Epoch 808/1000\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.5800 - acc: 0.7770 - val_loss: 0.8182 - val_acc: 0.7158\n",
      "Epoch 809/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5791 - acc: 0.7775 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 810/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5732 - acc: 0.7785 - val_loss: 0.8177 - val_acc: 0.7142\n",
      "Epoch 811/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5755 - acc: 0.7835 - val_loss: 0.8182 - val_acc: 0.7167\n",
      "Epoch 812/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5801 - acc: 0.7758 - val_loss: 0.8168 - val_acc: 0.7142\n",
      "Epoch 813/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5801 - acc: 0.7772 - val_loss: 0.8177 - val_acc: 0.7133\n",
      "Epoch 814/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5796 - acc: 0.7825 - val_loss: 0.8178 - val_acc: 0.7133\n",
      "Epoch 815/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5775 - acc: 0.7798 - val_loss: 0.8175 - val_acc: 0.7150\n",
      "Epoch 816/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5785 - acc: 0.7818 - val_loss: 0.8176 - val_acc: 0.7117\n",
      "Epoch 817/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5745 - acc: 0.7775 - val_loss: 0.8170 - val_acc: 0.7158\n",
      "Epoch 818/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5798 - acc: 0.7775 - val_loss: 0.8177 - val_acc: 0.7142\n",
      "Epoch 819/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5806 - acc: 0.7738 - val_loss: 0.8172 - val_acc: 0.7133\n",
      "Epoch 820/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5822 - acc: 0.7770 - val_loss: 0.8168 - val_acc: 0.7150\n",
      "Epoch 821/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5803 - acc: 0.7785 - val_loss: 0.8170 - val_acc: 0.7150\n",
      "Epoch 822/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5784 - acc: 0.7800 - val_loss: 0.8181 - val_acc: 0.7142\n",
      "Epoch 823/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5729 - acc: 0.7818 - val_loss: 0.8173 - val_acc: 0.7150\n",
      "Epoch 824/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5786 - acc: 0.7823 - val_loss: 0.8171 - val_acc: 0.7150\n",
      "Epoch 825/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5774 - acc: 0.7788 - val_loss: 0.8174 - val_acc: 0.7133\n",
      "Epoch 826/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5734 - acc: 0.7798 - val_loss: 0.8168 - val_acc: 0.7142\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5817 - acc: 0.7775 - val_loss: 0.8175 - val_acc: 0.7117\n",
      "Epoch 828/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5787 - acc: 0.7762 - val_loss: 0.8170 - val_acc: 0.7158\n",
      "Epoch 829/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5741 - acc: 0.7870 - val_loss: 0.8167 - val_acc: 0.7175\n",
      "Epoch 830/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5793 - acc: 0.7812 - val_loss: 0.8166 - val_acc: 0.7142\n",
      "Epoch 831/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5756 - acc: 0.7777 - val_loss: 0.8166 - val_acc: 0.7142\n",
      "Epoch 832/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5785 - acc: 0.7848 - val_loss: 0.8171 - val_acc: 0.7158\n",
      "Epoch 833/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5788 - acc: 0.7772 - val_loss: 0.8166 - val_acc: 0.7167\n",
      "Epoch 834/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5701 - acc: 0.7840 - val_loss: 0.8173 - val_acc: 0.7158\n",
      "Epoch 835/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5790 - acc: 0.7772 - val_loss: 0.8168 - val_acc: 0.7183\n",
      "Epoch 836/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5822 - acc: 0.7777 - val_loss: 0.8176 - val_acc: 0.7150\n",
      "Epoch 837/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5740 - acc: 0.7833 - val_loss: 0.8165 - val_acc: 0.7183\n",
      "Epoch 838/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5801 - acc: 0.7792 - val_loss: 0.8171 - val_acc: 0.7158\n",
      "Epoch 839/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5783 - acc: 0.7825 - val_loss: 0.8173 - val_acc: 0.7117\n",
      "Epoch 840/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5820 - acc: 0.7782 - val_loss: 0.8182 - val_acc: 0.7142\n",
      "Epoch 841/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5774 - acc: 0.7792 - val_loss: 0.8175 - val_acc: 0.7133\n",
      "Epoch 842/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5803 - acc: 0.7807 - val_loss: 0.8170 - val_acc: 0.7142\n",
      "Epoch 843/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5802 - acc: 0.7788 - val_loss: 0.8168 - val_acc: 0.7167\n",
      "Epoch 844/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5832 - acc: 0.7800 - val_loss: 0.8173 - val_acc: 0.7150\n",
      "Epoch 845/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5811 - acc: 0.7775 - val_loss: 0.8177 - val_acc: 0.7142\n",
      "Epoch 846/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5805 - acc: 0.7768 - val_loss: 0.8172 - val_acc: 0.7150\n",
      "Epoch 847/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5736 - acc: 0.7812 - val_loss: 0.8174 - val_acc: 0.7158\n",
      "Epoch 848/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5835 - acc: 0.7802 - val_loss: 0.8176 - val_acc: 0.7158\n",
      "Epoch 849/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5803 - acc: 0.7775 - val_loss: 0.8171 - val_acc: 0.7158\n",
      "Epoch 850/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5737 - acc: 0.7782 - val_loss: 0.8167 - val_acc: 0.7167\n",
      "Epoch 851/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5811 - acc: 0.7762 - val_loss: 0.8176 - val_acc: 0.7133\n",
      "Epoch 852/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5740 - acc: 0.7780 - val_loss: 0.8178 - val_acc: 0.7133\n",
      "Epoch 853/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5815 - acc: 0.7823 - val_loss: 0.8168 - val_acc: 0.7175\n",
      "Epoch 854/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5839 - acc: 0.7837 - val_loss: 0.8179 - val_acc: 0.7142\n",
      "Epoch 855/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5727 - acc: 0.7855 - val_loss: 0.8178 - val_acc: 0.7150\n",
      "Epoch 856/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5720 - acc: 0.7810 - val_loss: 0.8182 - val_acc: 0.7133\n",
      "Epoch 857/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5811 - acc: 0.7798 - val_loss: 0.8174 - val_acc: 0.7117\n",
      "Epoch 858/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5795 - acc: 0.7790 - val_loss: 0.8175 - val_acc: 0.7142\n",
      "Epoch 859/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5743 - acc: 0.7798 - val_loss: 0.8169 - val_acc: 0.7158\n",
      "Epoch 860/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5804 - acc: 0.7825 - val_loss: 0.8172 - val_acc: 0.7150\n",
      "Epoch 861/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5758 - acc: 0.7795 - val_loss: 0.8176 - val_acc: 0.7142\n",
      "Epoch 862/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5799 - acc: 0.7833 - val_loss: 0.8172 - val_acc: 0.7125\n",
      "Epoch 863/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5771 - acc: 0.7798 - val_loss: 0.8174 - val_acc: 0.7150\n",
      "Epoch 864/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5767 - acc: 0.7782 - val_loss: 0.8168 - val_acc: 0.7142\n",
      "Epoch 865/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5778 - acc: 0.7830 - val_loss: 0.8173 - val_acc: 0.7142\n",
      "Epoch 866/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5799 - acc: 0.7780 - val_loss: 0.8172 - val_acc: 0.7150\n",
      "Epoch 867/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5749 - acc: 0.7827 - val_loss: 0.8166 - val_acc: 0.7167\n",
      "Epoch 868/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5720 - acc: 0.7805 - val_loss: 0.8172 - val_acc: 0.7133\n",
      "Epoch 869/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5850 - acc: 0.7735 - val_loss: 0.8173 - val_acc: 0.7142\n",
      "Epoch 870/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5796 - acc: 0.7750 - val_loss: 0.8185 - val_acc: 0.7150\n",
      "Epoch 871/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5789 - acc: 0.7843 - val_loss: 0.8167 - val_acc: 0.7158\n",
      "Epoch 872/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5790 - acc: 0.7845 - val_loss: 0.8170 - val_acc: 0.7142\n",
      "Epoch 873/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5791 - acc: 0.7798 - val_loss: 0.8169 - val_acc: 0.7150\n",
      "Epoch 874/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5786 - acc: 0.7815 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 875/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5811 - acc: 0.7775 - val_loss: 0.8164 - val_acc: 0.7158\n",
      "Epoch 876/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5848 - acc: 0.7820 - val_loss: 0.8163 - val_acc: 0.7167\n",
      "Epoch 877/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5776 - acc: 0.7805 - val_loss: 0.8171 - val_acc: 0.7158\n",
      "Epoch 878/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5767 - acc: 0.7775 - val_loss: 0.8178 - val_acc: 0.7133\n",
      "Epoch 879/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5848 - acc: 0.7758 - val_loss: 0.8174 - val_acc: 0.7142\n",
      "Epoch 880/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5779 - acc: 0.7825 - val_loss: 0.8163 - val_acc: 0.7158\n",
      "Epoch 881/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5822 - acc: 0.7810 - val_loss: 0.8157 - val_acc: 0.7167\n",
      "Epoch 882/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5841 - acc: 0.7777 - val_loss: 0.8162 - val_acc: 0.7167\n",
      "Epoch 883/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5783 - acc: 0.7818 - val_loss: 0.8158 - val_acc: 0.7158\n",
      "Epoch 884/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5707 - acc: 0.7850 - val_loss: 0.8174 - val_acc: 0.7125\n",
      "Epoch 885/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5791 - acc: 0.7823 - val_loss: 0.8168 - val_acc: 0.7167\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5816 - acc: 0.7762 - val_loss: 0.8179 - val_acc: 0.7133\n",
      "Epoch 887/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5762 - acc: 0.7775 - val_loss: 0.8169 - val_acc: 0.7150\n",
      "Epoch 888/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5784 - acc: 0.7820 - val_loss: 0.8171 - val_acc: 0.7142\n",
      "Epoch 889/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5797 - acc: 0.7785 - val_loss: 0.8175 - val_acc: 0.7142\n",
      "Epoch 890/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5739 - acc: 0.7802 - val_loss: 0.8184 - val_acc: 0.7150\n",
      "Epoch 891/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5761 - acc: 0.7785 - val_loss: 0.8164 - val_acc: 0.7158\n",
      "Epoch 892/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5781 - acc: 0.7768 - val_loss: 0.8165 - val_acc: 0.7167\n",
      "Epoch 893/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5815 - acc: 0.7747 - val_loss: 0.8166 - val_acc: 0.7183\n",
      "Epoch 894/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5763 - acc: 0.7805 - val_loss: 0.8167 - val_acc: 0.7175\n",
      "Epoch 895/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5760 - acc: 0.7867 - val_loss: 0.8180 - val_acc: 0.7158\n",
      "Epoch 896/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5778 - acc: 0.7848 - val_loss: 0.8181 - val_acc: 0.7142\n",
      "Epoch 897/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5815 - acc: 0.7782 - val_loss: 0.8176 - val_acc: 0.7125\n",
      "Epoch 898/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5805 - acc: 0.7812 - val_loss: 0.8172 - val_acc: 0.7158\n",
      "Epoch 899/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5724 - acc: 0.7812 - val_loss: 0.8170 - val_acc: 0.7150\n",
      "Epoch 900/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5821 - acc: 0.7760 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 901/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5744 - acc: 0.7772 - val_loss: 0.8172 - val_acc: 0.7158\n",
      "Epoch 902/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5773 - acc: 0.7820 - val_loss: 0.8167 - val_acc: 0.7142\n",
      "Epoch 903/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5749 - acc: 0.7850 - val_loss: 0.8171 - val_acc: 0.7150\n",
      "Epoch 904/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5781 - acc: 0.7788 - val_loss: 0.8177 - val_acc: 0.7142\n",
      "Epoch 905/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5733 - acc: 0.7788 - val_loss: 0.8174 - val_acc: 0.7133\n",
      "Epoch 906/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5790 - acc: 0.7807 - val_loss: 0.8177 - val_acc: 0.7125\n",
      "Epoch 907/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5753 - acc: 0.7820 - val_loss: 0.8181 - val_acc: 0.7125\n",
      "Epoch 908/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5709 - acc: 0.7833 - val_loss: 0.8166 - val_acc: 0.7175\n",
      "Epoch 909/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5753 - acc: 0.7768 - val_loss: 0.8166 - val_acc: 0.7175\n",
      "Epoch 910/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5773 - acc: 0.7768 - val_loss: 0.8160 - val_acc: 0.7158\n",
      "Epoch 911/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5761 - acc: 0.7833 - val_loss: 0.8168 - val_acc: 0.7175\n",
      "Epoch 912/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5812 - acc: 0.7810 - val_loss: 0.8172 - val_acc: 0.7150\n",
      "Epoch 913/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5747 - acc: 0.7802 - val_loss: 0.8158 - val_acc: 0.7183\n",
      "Epoch 914/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5776 - acc: 0.7812 - val_loss: 0.8182 - val_acc: 0.7133\n",
      "Epoch 915/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5738 - acc: 0.7762 - val_loss: 0.8167 - val_acc: 0.7158\n",
      "Epoch 916/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5761 - acc: 0.7873 - val_loss: 0.8179 - val_acc: 0.7158\n",
      "Epoch 917/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5801 - acc: 0.7782 - val_loss: 0.8173 - val_acc: 0.7167\n",
      "Epoch 918/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5751 - acc: 0.7837 - val_loss: 0.8166 - val_acc: 0.7167\n",
      "Epoch 919/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5785 - acc: 0.7785 - val_loss: 0.8176 - val_acc: 0.7125\n",
      "Epoch 920/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5765 - acc: 0.7815 - val_loss: 0.8175 - val_acc: 0.7158\n",
      "Epoch 921/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5739 - acc: 0.7870 - val_loss: 0.8169 - val_acc: 0.7158\n",
      "Epoch 922/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5790 - acc: 0.7833 - val_loss: 0.8175 - val_acc: 0.7158\n",
      "Epoch 923/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5710 - acc: 0.7860 - val_loss: 0.8164 - val_acc: 0.7183\n",
      "Epoch 924/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5762 - acc: 0.7802 - val_loss: 0.8167 - val_acc: 0.7167\n",
      "Epoch 925/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5797 - acc: 0.7812 - val_loss: 0.8171 - val_acc: 0.7158\n",
      "Epoch 926/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5749 - acc: 0.7820 - val_loss: 0.8169 - val_acc: 0.7167\n",
      "Epoch 927/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5786 - acc: 0.7790 - val_loss: 0.8182 - val_acc: 0.7133\n",
      "Epoch 928/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5667 - acc: 0.7920 - val_loss: 0.8173 - val_acc: 0.7158\n",
      "Epoch 929/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5800 - acc: 0.7818 - val_loss: 0.8160 - val_acc: 0.7167\n",
      "Epoch 930/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5757 - acc: 0.7798 - val_loss: 0.8170 - val_acc: 0.7150\n",
      "Epoch 931/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5738 - acc: 0.7792 - val_loss: 0.8169 - val_acc: 0.7167\n",
      "Epoch 932/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5764 - acc: 0.7795 - val_loss: 0.8177 - val_acc: 0.7150\n",
      "Epoch 933/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5724 - acc: 0.7855 - val_loss: 0.8177 - val_acc: 0.7150\n",
      "Epoch 934/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5751 - acc: 0.7827 - val_loss: 0.8169 - val_acc: 0.7142\n",
      "Epoch 935/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5719 - acc: 0.7883 - val_loss: 0.8184 - val_acc: 0.7142\n",
      "Epoch 936/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5721 - acc: 0.7855 - val_loss: 0.8170 - val_acc: 0.7167\n",
      "Epoch 937/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5789 - acc: 0.7788 - val_loss: 0.8175 - val_acc: 0.7150\n",
      "Epoch 938/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5741 - acc: 0.7795 - val_loss: 0.8180 - val_acc: 0.7150\n",
      "Epoch 939/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5703 - acc: 0.7805 - val_loss: 0.8170 - val_acc: 0.7167\n",
      "Epoch 940/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5760 - acc: 0.7770 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 941/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5807 - acc: 0.7760 - val_loss: 0.8175 - val_acc: 0.7133\n",
      "Epoch 942/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5758 - acc: 0.7825 - val_loss: 0.8173 - val_acc: 0.7133\n",
      "Epoch 943/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5779 - acc: 0.7853 - val_loss: 0.8176 - val_acc: 0.7133\n",
      "Epoch 944/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5786 - acc: 0.7780 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5775 - acc: 0.7807 - val_loss: 0.8177 - val_acc: 0.7133\n",
      "Epoch 946/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5784 - acc: 0.7820 - val_loss: 0.8168 - val_acc: 0.7167\n",
      "Epoch 947/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5765 - acc: 0.7825 - val_loss: 0.8163 - val_acc: 0.7158\n",
      "Epoch 948/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5791 - acc: 0.7735 - val_loss: 0.8172 - val_acc: 0.7142\n",
      "Epoch 949/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5736 - acc: 0.7777 - val_loss: 0.8171 - val_acc: 0.7150\n",
      "Epoch 950/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5744 - acc: 0.7810 - val_loss: 0.8173 - val_acc: 0.7150\n",
      "Epoch 951/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5830 - acc: 0.7750 - val_loss: 0.8166 - val_acc: 0.7167\n",
      "Epoch 952/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5733 - acc: 0.7795 - val_loss: 0.8177 - val_acc: 0.7142\n",
      "Epoch 953/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5707 - acc: 0.7823 - val_loss: 0.8172 - val_acc: 0.7158\n",
      "Epoch 954/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5783 - acc: 0.7818 - val_loss: 0.8181 - val_acc: 0.7133\n",
      "Epoch 955/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5724 - acc: 0.7823 - val_loss: 0.8179 - val_acc: 0.7142\n",
      "Epoch 956/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5746 - acc: 0.7790 - val_loss: 0.8185 - val_acc: 0.7142\n",
      "Epoch 957/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5695 - acc: 0.7853 - val_loss: 0.8182 - val_acc: 0.7142\n",
      "Epoch 958/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5738 - acc: 0.7833 - val_loss: 0.8175 - val_acc: 0.7133\n",
      "Epoch 959/1000\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.5773 - acc: 0.7807 - val_loss: 0.8170 - val_acc: 0.7150\n",
      "Epoch 960/1000\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.5755 - acc: 0.7760 - val_loss: 0.8181 - val_acc: 0.7150\n",
      "Epoch 961/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5753 - acc: 0.7798 - val_loss: 0.8176 - val_acc: 0.7133\n",
      "Epoch 962/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5744 - acc: 0.7815 - val_loss: 0.8175 - val_acc: 0.7117\n",
      "Epoch 963/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5789 - acc: 0.7827 - val_loss: 0.8173 - val_acc: 0.7158\n",
      "Epoch 964/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5777 - acc: 0.7795 - val_loss: 0.8175 - val_acc: 0.7133\n",
      "Epoch 965/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5800 - acc: 0.7802 - val_loss: 0.8172 - val_acc: 0.7133\n",
      "Epoch 966/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5780 - acc: 0.7800 - val_loss: 0.8172 - val_acc: 0.7133\n",
      "Epoch 967/1000\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.5777 - acc: 0.7792 - val_loss: 0.8172 - val_acc: 0.7133\n",
      "Epoch 968/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5769 - acc: 0.7835 - val_loss: 0.8176 - val_acc: 0.7142\n",
      "Epoch 969/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5704 - acc: 0.7820 - val_loss: 0.8165 - val_acc: 0.7150\n",
      "Epoch 970/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5728 - acc: 0.7790 - val_loss: 0.8171 - val_acc: 0.7133\n",
      "Epoch 971/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5784 - acc: 0.7785 - val_loss: 0.8174 - val_acc: 0.7142\n",
      "Epoch 972/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5778 - acc: 0.7812 - val_loss: 0.8171 - val_acc: 0.7167\n",
      "Epoch 973/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5768 - acc: 0.7815 - val_loss: 0.8169 - val_acc: 0.7167\n",
      "Epoch 974/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5744 - acc: 0.7860 - val_loss: 0.8165 - val_acc: 0.7158\n",
      "Epoch 975/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5753 - acc: 0.7833 - val_loss: 0.8172 - val_acc: 0.7142\n",
      "Epoch 976/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5792 - acc: 0.7760 - val_loss: 0.8171 - val_acc: 0.7158\n",
      "Epoch 977/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5798 - acc: 0.7768 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 978/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5732 - acc: 0.7893 - val_loss: 0.8179 - val_acc: 0.7133\n",
      "Epoch 979/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5855 - acc: 0.7758 - val_loss: 0.8172 - val_acc: 0.7167\n",
      "Epoch 980/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5759 - acc: 0.7867 - val_loss: 0.8176 - val_acc: 0.7167\n",
      "Epoch 981/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5788 - acc: 0.7823 - val_loss: 0.8178 - val_acc: 0.7142\n",
      "Epoch 982/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5786 - acc: 0.7750 - val_loss: 0.8177 - val_acc: 0.7142\n",
      "Epoch 983/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5769 - acc: 0.7850 - val_loss: 0.8175 - val_acc: 0.7142\n",
      "Epoch 984/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5720 - acc: 0.7857 - val_loss: 0.8180 - val_acc: 0.7175\n",
      "Epoch 985/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5740 - acc: 0.7768 - val_loss: 0.8176 - val_acc: 0.7150\n",
      "Epoch 986/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5727 - acc: 0.7802 - val_loss: 0.8171 - val_acc: 0.7167\n",
      "Epoch 987/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5773 - acc: 0.7732 - val_loss: 0.8173 - val_acc: 0.7175\n",
      "Epoch 988/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5767 - acc: 0.7825 - val_loss: 0.8185 - val_acc: 0.7158\n",
      "Epoch 989/1000\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.5746 - acc: 0.7742 - val_loss: 0.8171 - val_acc: 0.7158\n",
      "Epoch 990/1000\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.5751 - acc: 0.7812 - val_loss: 0.8177 - val_acc: 0.7142\n",
      "Epoch 991/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.5812 - acc: 0.7795 - val_loss: 0.8176 - val_acc: 0.7133\n",
      "Epoch 992/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5818 - acc: 0.7790 - val_loss: 0.8176 - val_acc: 0.7142\n",
      "Epoch 993/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5735 - acc: 0.7792 - val_loss: 0.8164 - val_acc: 0.7167\n",
      "Epoch 994/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5723 - acc: 0.7815 - val_loss: 0.8165 - val_acc: 0.7167\n",
      "Epoch 995/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5778 - acc: 0.7827 - val_loss: 0.8156 - val_acc: 0.7175\n",
      "Epoch 996/1000\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.5772 - acc: 0.7785 - val_loss: 0.8169 - val_acc: 0.7167\n",
      "Epoch 997/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5785 - acc: 0.7835 - val_loss: 0.8167 - val_acc: 0.7175\n",
      "Epoch 998/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5696 - acc: 0.7855 - val_loss: 0.8169 - val_acc: 0.7167\n",
      "Epoch 999/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5803 - acc: 0.7823 - val_loss: 0.8176 - val_acc: 0.7142\n",
      "Epoch 1000/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.5733 - acc: 0.7812 - val_loss: 0.8167 - val_acc: 0.7175\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "epochs = 1000 \n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    epochs=epochs,  \n",
    "    batch_size=batch_size, \n",
    "    validation_data=(validation_data, validation_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('InceptionV4/InceptionV4_FE_train_1.npy',  train_features)\n",
    "np.save('InceptionV4/InceptionV4_FE_validation_1.npy',  validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5py \n",
    "model.save_weights('InceptionV4/InceptionV4_FE_model_1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graphics (history):\n",
    "    #Mostramos otro tipo de grafico\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc)+ 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    #plt.tittle('Trainning and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    #plt.tittle('Trainning and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluate_model(model, history):\n",
    "    #Evaluate model\n",
    "    (loss, acc) = model.evaluate(\n",
    "        test_features, test_labels, \n",
    "        batch_size=batch_size, \n",
    "        verbose=0)\n",
    "\n",
    "    print(\"acc: {0:.2f}% - loss: {1:f}\".format(acc * 100, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 64.25% - loss: 0.864543\n"
     ]
    }
   ],
   "source": [
    "print_evaluate_model(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7acfbc5048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3Xt8VNW99/HPj5CQcpFLkHoJIaC0EO4xgh6kiCiiPkqx1AOC4qVS0arHHk+LRavShx718FLUh0O1LVQlhYdHjxYV5VilpbZHJVAKchMU0IByiYIXUEiynj/WTDKZXGYmGTJJ9vf9eq3XzN6z9t5rz575rcves8ecc4iISDC0SnUBRESk8Sjoi4gEiIK+iEiAKOiLiASIgr6ISIAo6IuIBIiCvohIgCjoi4gEiIK+iEiAtE51AaJ17drV5ebmproYIiLNypo1aw44506Mla/JBf3c3FyKiopSXQwRkWbFzHbFk0/DOyIiAaKgLyISIAr6IiIBoqAvIhIgCvoiIgGioC8iEiAK+iIiAdKigr5zcPQolJbCF19Uf/3LL/3j0aPxr/PIEb/e6O2UldW/nE2Vc7BoEWzYAJ9/nurSiMjx0OR+nFVfn34KN98MixdD585++pFH4He/gwED4Kmnqub/4Q/hwAGYOBGWLYMpU+Ctt+Avf4Fx42DuXMjNhT/+0ee/4gq44QZ47TVYuRL+8Q8YOxb69YN//3c46SRo1Qrmz4dvfhP27oXdu6GkBC69FPLz4fTTYepU6N0b5s3z5br8cti0yee/7z6/rW9/G84/H957DxYuhDfegG7dfCDOzoasLDj3XLjoIhg0CE4+GZYvh6efhn/6J/jVryAnB667zq/3rrtg1y6/jj17/Pb794dbboFevXyQ//BDX5af/rTyPdq/H7p2rfq+OedTebl/DwYOhNWrISMDPvkEOnaEYcPg2DHYvh369oUnn/TPv/tdP922bdV1lpb6x9at/XMzOHTIH4Pp0/17u2CBP06LF1cu//HH8PXX8NVXsG8fjBhR/XNRXu6PS9h778Gpp0Jmpt+Pxx+Hs86CwYP962VlkJYW54fuODl2zDdQOnXyDZT0dP+egH9fTjihclokUdbU/hi9oKDA1ecXuQcP+mAvydWmjQ+sp50G11wDd98de5ncXNi5s/bXf/ITeOkl2LjRB/SPP05OWXv0gC5dfOVSXl5Zid5/v39t0qSq+cOVM0D79vCNb/hgmp8P55zjK/B27Xzl3r+/rwCd85Xb2LG+Aj3tNF9ZTp3qK7uVK6GoCMaPh7PPhh07fAWzeDGMGuV7jvn5fj0LF/r8XbvCK6/4CvTuu+GSS6qW85e/hFtv9e/XsGHwzDO+XP36QffuvrJfv97v544d8O67vjLft89Xxh06+Eq9Vy/4619h+HBf8W/cCO+847f9yiu+olm9Gvr08eUz85Xk22/7hsiAAbBlC3z2md+HzMzKysc5+PvfIS/Pf06uv95Xpm3b+jxlZb5S37LFrwsqK+OjR305a+KcrwRre72pirfxEH6fk8HM1jjnCmLmaylBH3zL/bPP/Bd/xQr4wQ986/zQIRg61Lfgt271gWbPHv+Gz5njP9jvvgvf+Y7/Ep18sv9AHj4Mv/mNDxbdu/uK5YQT/PMLLvBfxq+/9pXN3r1+mWPH/Jdm2TL/Rbz9dp9n717429/go48qy5uTAx984APHaafBf/935Wu9e/sgtH69n77sMv+F+vJLeOwxv62HHvKt8dmzfZ7nn/cBrybDhvmeTF6e71lkZfngdd99fttHj/ogd889vkcxZUplCzwR3br5L/eePVXnn366b+0nIjPTt+LD8vNh7drEy1Sb9HR/vJqzM8/0gToVLr0UCgp85TJvXuz8GRmVQ6tnnul7lU895b8rkyb5yq9vX195fPe7/liHGxnt28Ntt/mKa906/zkeNgyGDPHf3xUr4PXXfeV78KA/rv/xH3Diif47mJ3te97//M/+M969u6/EjhyB3/7Wf3dmzPAV9lln+c/ec8/BAw/4HueaNTBrlq/877kHRo70owhvvun35eKL/fpGj4Yrr4SXX/brePBB3wM9cgS+9S1f6c2f79+LoiLfy37gAV+p79jhv4v1reACGfQborTUB6vGcOyYDzg1+fxz3zILc85XZB07xrfuQ4f8l2DdOpgwwbe0ysp8iz0Re/f6L3PHjv7x5JN9ZTFypK/EDh3ylefcub6CGjLEV7bt2/vly8p8nvJy/4W88kpf6e3b54dqvvc9P4SVnV15juTjj+HVV2HpUv9lP/ts/0Xt2NFXbr16+eGyO+/0+zZ/Psyc6d+znBz/vm3Z4r+AI0fCmDH+S9Wtmx/++uwzmDbNf/k++ghuusnnP3DAl3P6dL8vN9zg9z8tzZfxwAH/hX/uufjeu1NOqVrphSuvQYN8K/fIEXjhBf/aCy/4+Tk5fnrcOL/fL77oy/Tll34YLREXXeTf8/Ly6q+NHAl//nNi65PGM2WK77nVh4K+SJIdOeJ7X+ArgqwsKC72jYX33/ctxMOHfeVTWgpPPAFXXeUr3I0bfcVY2/rKy303v7au/h/+4Ht355xTOYz59de+0r3ggsqe6Te/6VuL4TL+5Ce+xfvJJ35I6JRTfAW6b59vqd58s694t2/3leNXX/nKLvzYubN/bNPG90QXLvTDNxdeCH/6E6xa5c8vmfnzSWvX+vMj5eW+ku3WzZ9P2rnT5/vlL/02MzJ85b1+vU8HDvgyL1ni1/HVV/58Wpcufru33ebL+dxz/j3bubNqrxngxhv9cFVenh8+HDPGN0xefdUPb7Vu7SvRgwdrfo87dvQNgRdf9O/TkSN+/+rSvbsfRbjwQvj5z33DpUMH30BasMA3MN58059f7NbNN/juvNOXbdUqX6H/9reV6xs71jdQ6jPko6AvIs3CZ5/5oNyvX+Ns7+hR3zucM8dXLg89VDm2HhlsnfMXURQV+V5gZmblkOebb/oKOHq9rVtXvXAgHiUlcO+9ftgo+sKJRCjoi4gESLxBv0Vdpy8iInVT0BcRCRAFfRGRAFHQFxEJEAV9EZEAUdAXEQmQuIK+mY01s61mtt3MZtTweo6ZrTSzv5vZejO7OOK1O0PLbTWzC5NZeBERSUzMGw+YWRowD7gAKAZWm9ky59ymiGx3AUudc/PNLA9YDuSGnk8E+gGnAH80s28551rgjYlFRJq+eFr6Q4Htzrn3nXNHgSXAuKg8Djgh9LwjEL7zyDhgiXPua+fcDmB7aH0iIpIC8QT9U4EPI6aLQ/Mi3QtMMbNifCv/lgSWFRGRRpKsE7mTgN8557KBi4GnzSzudZvZNDMrMrOi/fv3J6lIIiISLZ7AvBvoHjGdHZoX6XpgKYBz7n+ATKBrnMvinHvCOVfgnCs48cQT4y+9iIgkJJ6gvxrobWY9zSwDf2J2WVSeD4DRAGbWFx/094fyTTSzNmbWE+gNvJ2swouISGJiXr3jnCs1sx8BK4A0YIFzbqOZzQKKnHPLgH8Ffm1mt+NP6l7j/O07N5rZUmATUArcrCt3RERSR7dWFhFpAXRrZRERqUZBX0QkQBT0RUQCREFfRCRAFPRFRAJEQV9EJEAU9EVEAkRBX0QkQBT0RUQCREFfRCRAFPRFRAJEQV9EJEAU9EVEAkRBX0QkQBT0RUQCREFfRCRAFPRFRAJEQV9EJEAU9EVEAkRBX0QkQBT0RUQCREFfRCRAFPRFRAJEQV9EJEAU9EVEAkRBX0QkQBT0RUQCREFfRCRAFPRFRAJEQV9EJEDiCvpmNtbMtprZdjObUcPrD5vZulB618wORrxWFvHasmQWXkREEtM6VgYzSwPmARcAxcBqM1vmnNsUzuOcuz0i/y3AkIhVHHHODU5ekUVEpL7iaekPBbY75953zh0FlgDj6sg/CVicjMKJiEhyxRP0TwU+jJguDs2rxsx6AD2B1yNmZ5pZkZm9aWbfrXdJRUSkwWIO7yRoIvCMc64sYl4P59xuM+sFvG5mG5xz70UuZGbTgGkAOTk5SS6SiIiExdPS3w10j5jODs2ryUSihnacc7tDj+8Df6LqeH84zxPOuQLnXMGJJ54YR5FERKQ+4gn6q4HeZtbTzDLwgb3aVThm1gfoDPxPxLzOZtYm9LwrMBzYFL2siIg0jpjDO865UjP7EbACSAMWOOc2mtksoMg5F64AJgJLnHMuYvG+wONmVo6vYO6PvOpHREQal1WN0alXUFDgioqKUl0MEZFmxczWOOcKYuXTL3JFRAJEQV9EJEAU9EVEAkRBX0QkQBT0RUQCREFfRCRAFPRFRAJEQV9EJEAU9EVEAkRBX0QkQBT0RUQCREFfRCRAFPRFRAJEQV9EJEAU9EVEAkRBX0QkQBT0RUQCREFfRCRAFPRFRAJEQV9EJEAU9EVEAkRBX0QkQFqnugAiklrHjh2juLiYr776KtVFkThkZmaSnZ1Nenp6vZZX0BcJuOLiYjp06EBubi5mluriSB2cc5SUlFBcXEzPnj3rtQ4N74gE3FdffUVWVpYCfjNgZmRlZTWoV6agLyIK+M1IQ4+Vgr6IpFRJSQmDBw9m8ODBnHTSSZx66qkV00ePHo1rHddeey1bt26tM8+8efMoLCxMRpE555xzWLduXVLW1dg0pi8iCSkshJkz4YMPICcHZs+GyZPrv76srKyKAHrvvffSvn177rjjjip5nHM452jVquZ26sKFC2Nu5+abb65/IVsQtfRFJG6FhTBtGuzaBc75x2nT/Pxk2759O3l5eUyePJl+/frx0UcfMW3aNAoKCujXrx+zZs2qyBtueZeWltKpUydmzJjBoEGDOPvss9m3bx8Ad911F3Pnzq3IP2PGDIYOHcq3v/1t/va3vwHw5Zdf8r3vfY+8vDwmTJhAQUFBzBb9okWLGDBgAP379+dnP/sZAKWlpVx11VUV8x999FEAHn74YfLy8hg4cCBTpkxJ+nsWD7X0RSRuM2fC4cNV5x0+7Oc3pLVfmy1btvDUU09RUFAAwP3330+XLl0oLS1l1KhRTJgwgby8vCrLHDp0iJEjR3L//ffz4x//mAULFjBjxoxq63bO8fbbb7Ns2TJmzZrFK6+8wmOPPcZJJ53Es88+yz/+8Q/y8/PrLF9xcTF33XUXRUVFdOzYkfPPP58XX3yRE088kQMHDrBhwwYADh48CMCDDz7Irl27yMjIqJjX2OJq6ZvZWDPbambbzazau2dmD5vZulB618wORrw21cy2hdLUZBZeRBrXBx8kNr+hTjvttIqAD7B48WLy8/PJz89n8+bNbNq0qdoy3/jGN7jooosAOOOMM9i5c2eN67788sur5XnjjTeYOHEiAIMGDaJfv351lu+tt97ivPPOo2vXrqSnp3PllVeyatUqTj/9dLZu3cqtt97KihUr6NixIwD9+vVjypQpFBYW1vs6+4aKGfTNLA2YB1wE5AGTzKxK1eqcu905N9g5Nxh4DPiv0LJdgHuAYcBQ4B4z65zcXRCRxpKTk9j8hmrXrl3F823btvHII4/w+uuvs379esaOHVvjpYsZGRkVz9PS0igtLa1x3W3atImZp76ysrJYv349I0aMYN68efzwhz8EYMWKFdx4442sXr2aoUOHUlZWltTtxiOelv5QYLtz7n3n3FFgCTCujvyTgMWh5xcCrzrnPnHOfQq8CoxtSIFFJHVmz4a2bavOa9vWzz/ePvvsMzp06MAJJ5zARx99xIoVK5K+jeHDh7N06VIANmzYUGNPItKwYcNYuXIlJSUllJaWsmTJEkaOHMn+/ftxzvH973+fWbNmsXbtWsrKyiguLua8887jwQcf5MCBAxyOHitrBPGM6Z8KfBgxXYxvuVdjZj2AnsDrdSx7auLFFJGmIDxun8yrd+KVn59PXl4effr0oUePHgwfPjzp27jlllu4+uqrycvLq0jhoZmaZGdn84tf/IJzzz0X5xyXXnopl1xyCWvXruX666/HOYeZ8cADD1BaWsqVV17J559/Tnl5OXfccQcdOnRI+j7EYs65ujOYTQDGOud+EJq+ChjmnPtRDXl/CmQ7524JTd8BZDrn/ndo+m7giHNuTtRy04BpADk5OWfs2rWrwTsmIvHZvHkzffv2TXUxmoTS0lJKS0vJzMxk27ZtjBkzhm3bttG6ddO65qWmY2Zma5xzBbUsUiGePdkNdI+Yzg7Nq8lEIPJi2N3AuVHL/il6IefcE8ATAAUFBXXXQiIix8kXX3zB6NGjKS0txTnH448/3uQCfkPFszergd5m1hMfxCcCV0ZnMrM+QGfgfyJmrwB+GXHydgxwZ4NKLCJynHTq1Ik1a9akuhjHVcyg75wrNbMf4QN4GrDAObfRzGYBRc65ZaGsE4ElLmK8yDn3iZn9Al9xAMxyzn2S3F0QEZF4xdVvcc4tB5ZHzft51PS9tSy7AFhQz/KJiEgS6TYMIiIBoqAvIhIgCvoiklKjRo2q9kOruXPnMn369DqXa9++PQB79uxhwoQJNeY599xzKSoqqnM9c+fOrfIjqYsvvjgp98W59957mTNnTuyMjUxBX0RSatKkSSxZsqTKvCVLljBp0qS4lj/llFN45pln6r396KC/fPlyOnXqVO/1NXUK+iKSUhMmTOCll16q+MOUnTt3smfPHkaMGFFx3Xx+fj4DBgzgD3/4Q7Xld+7cSf/+/QE4cuQIEydOpG/fvowfP54jR45U5Js+fXrFbZnvueceAB599FH27NnDqFGjGDVqFAC5ubkcOHAAgIceeoj+/fvTv3//itsy79y5k759+3LDDTfQr18/xowZU2U7NVm3bh1nnXUWAwcOZPz48Xz66acV2w/fajl8o7c///nPFX8iM2TIED7//PN6v7c1aVm/OhCRBvmXf4Fk/yHU4MEQipc16tKlC0OHDuXll19m3LhxLFmyhCuuuAIzIzMzk+eee44TTjiBAwcOcNZZZ3HZZZfV+peB8+fPp23btmzevJn169dXuTXy7Nmz6dKlC2VlZYwePZr169dz66238tBDD7Fy5Uq6du1aZV1r1qxh4cKFvPXWWzjnGDZsGCNHjqRz585s27aNxYsX8+tf/5orrriCZ599ts7741999dU89thjjBw5kp///Ofcd999zJ07l/vvv58dO3bQpk2biiGlOXPmMG/ePIYPH84XX3xBZmZmAu92bGrpi0jKRQ7xRA7tOOf42c9+xsCBAzn//PPZvXs3e/furXU9q1atqgi+AwcOZODAgRWvLV26lPz8fIYMGcLGjRtj3kztjTfeYPz48bRr14727dtz+eWX85e//AWAnj17MnjwYKDu2zeDv7//wYMHGTlyJABTp05l1apVFWWcPHkyixYtqvjl7/Dhw/nxj3/Mo48+ysGDB5P+i2C19EWkQl0t8uNp3Lhx3H777axdu5bDhw9zxhlnAFBYWMj+/ftZs2YN6enp5Obm1ng75Vh27NjBnDlzWL16NZ07d+aaa66p13rCwrdlBn9r5ljDO7V56aWXWLVqFS+88AKzZ89mw4YNzJgxg0suuYTly5czfPhwVqxYQZ8+fepd1mhq6YtIyrVv355Ro0Zx3XXXVTmBe+jQIbp160Z6ejorV64k1s0Yv/Od7/D73/8egHfeeYf169cD/rbM7dq1o2PHjuzdu5eXX365YpkOHTrUOG4+YsQInn/+eQ4fPsyXX37Jc889x4gRIxLet44dO9K5c+eKXsLTTz/NyJEjKS8v58MPP2TUqFE88MADHDp0iC+++IL33nuPAQMG8NOf/pQzzzyTLVu2JLzNuqilLyJNwqRJkxg/fnyVK3kmT57MpZdeyoABAygoKIjZ4p0+fTrXXnstffv2pW/fvhU9hkGDBjFkyBD69OlD9+7dq9yWedq0aYwdO5ZTTjmFlStXVszPz8/nmmuuYejQoQD84Ac/YMiQIXUO5dTmySef5MYbb+Tw4cP06tWLhQsXUlZWxpQpUzh06BDOOW699VY6derE3XffzcqVK2nVqhX9+vWr+BewZIl5a+XGVlBQ4GJdVysiyaNbKzc/Dbm1soZ3REQCREFfRCRAFPRFRAJEQV9EaGrn9qR2DT1WCvoiAZeZmUlJSYkCfzPgnKOkpKRBv9LVJZsiAZednU1xcTH79+9PdVEkDpmZmWRnZ9d7eQV9kYBLT0+nZ8+eqS6GNBIN74iIBIiCvohIgCjoi4gEiIK+iEiAKOiLiASIgr6ISIAo6IuIBIiCvohIgCjoi4gEiIK+iEiAKOiLiASIgr6ISIDEFfTNbKyZbTWz7WY2o5Y8V5jZJjPbaGa/j5hfZmbrQmlZsgouIiKJi3mXTTNLA+YBFwDFwGozW+ac2xSRpzdwJzDcOfepmXWLWMUR59zgJJdbRETqIZ6W/lBgu3PufefcUWAJMC4qzw3APOfcpwDOuX3JLaaIiCRDPEH/VODDiOni0LxI3wK+ZWZ/NbM3zWxsxGuZZlYUmv/dmjZgZtNCeYr0Rw4iIsdPsv5EpTXQGzgXyAZWmdkA59xBoIdzbreZ9QJeN7MNzrn3Ihd2zj0BPAFQUFCg/2wTETlO4mnp7wa6R0xnh+ZFKgaWOeeOOed2AO/iKwGcc7tDj+8DfwKGNLDMIiJST/EE/dVAbzPraWYZwEQg+iqc5/GtfMysK364530z62xmbSLmDwc2ISIiKRFzeMc5V2pmPwJWAGnAAufcRjObBRQ555aFXhtjZpuAMuDfnHMlZvZPwONmVo6vYO6PvOpHREQalznXtIbQCwoKXFFRUaqLISLSrJjZGudcQax8+kWuiEiAKOiLiASIgr6ISIAo6IuIBIiCvohIgCjoi4gEiIK+iEiAKOiLiASIgr6ISIAo6IuIBIiCvohIgCjoi4gEiIK+iEiAKOiLiASIgr6ISIAo6IuIBIiCvohIgCjoi4gEiIK+iEiAKOiLiASIgr6ISIAo6IuIBIiCvohIgCjoi4gESOCCfmEh5OZCq1b+sbAw1SUSEWk8gQn6hYXQtStMmQK7doFz/nHatLoDvyoJEWlJWlTQry1AFxb64F5SUn2Zw4fhttvqXi6RSkJEpClrMUH/ppvgqquqBuirrvLzZ870wb02JSVVl7vuOh/Ya1ouXEnEQ70EEWlqWkTQLyyEX/3KB+1IzsH8+T6QJ+LoUR/YP/ig5tdLSsDMDxd17VpzUFcvQUSaoriCvpmNNbOtZrbdzGbUkucKM9tkZhvN7PcR86ea2bZQmpqsgkeaObN6wG+omoaCaspTUlIZ1KdMgfbtK88d1NZLCJ9fMPMpvMzx7hHcdBO0bu232bq1nxaRgHHO1ZmANOA9oBeQAfwDyIvK0xv4O9A5NN0t9NgFeD/02Dn0vHNd2zvjjDNcosyc86G35aSsLOcWLap9nxctcq5HD7/vPXrUndc556ZPr3k706cn/HaLSBMEFLkY8dw5F1dLfyiw3Tn3vnPuKLAEGBeV5wZgnnPu01BFsi80/0LgVefcJ6HXXgXGJlwzxZCTk+w1pl5JiT8nEW6Vm1X2AmobOrrppurnEMLnFebPr3k7TzxR8/x4z0e0tPMWLW1/RKqJVSsAE4DfRExfBfyfqDzPAw8CfwXeBMaG5t8B3BWR727gjrq2V5+Wfm2tWKX4exVZWZW9htGjq/ee2rat7E0sWuTz17SuyHz1kWgPJpkWLfLlT+b+iDQWktjSj0dr/BDPucAk4Ndm1inehc1smpkVmVnR/v37E9740qUJLyIRos9NvPaafx7p8GF/7uSmm/z5itrOeYTz1SX6nEZaWuWJ8euua/jJ77ou3a2rFV/b1Vqx9kekWYlVKwBnAysipu8E7ozK8yvg2ojp14Az8RXA4xHzHwcm1bW9+rT0U91SDlKK9/xJVpZz7dpVnV60yKf09MS2Ge6JRK+rJrW11qdPr31+bb2WyH0WaeqIs6VvPm/tzKw18C4wGtgNrAaudM5tjMgzNhTMp5pZV/xJ3cGAA9YA+aGsa4EznHOf1La9goICV1RUFG+dFdp+QtklRTIyoEOH+K6MimddCxb45zNn+strc3Lgiy9qXn+rVlBeXr9t9egBO3fWu6gijcLM1jjnCmLlizm845wrBX4ErAA2A0udcxvNbJaZXRbKtgIoMbNNwErg35xzJaHg/gt8RbEamFVXwK+vrKxkr1GOh6NHkxPww+u67bbqJ7RrW399Az74dUaeFI/3JG90/ppOtIs0uni6A42Z6jO8s2iRcxkZqR/6UGq5KS2t9s9Y5HBTXSe5o1P4JHFjnbw+nttJ5Ql48YhzeCdmhsZO9Qn6zukKHqXUpowM5/LykrOuyMogkXMZdeWt6VxHrHXGS1c9NQ2BC/o9eqT+i6+kdLxTRoYPptOn+94H+NZ1TSfYw3ljfT/CJ7Rra6nHasXXtu4ePRL/HqvHUH+BC/ot8Ve5SkrJSPEON0WmyN5GrFZ8Xd+9WAE8MshnZVUfQlOPIX6BC/q1tTaiLx1UUlJqeMrKiv3di64MwtPhCqC2IafoVJ8eQ1N1PHsygQv6sVok0W92ZHe2phZGZEpP9/kTvb68ppSWpl6JUstJkUNMiSzXtm1iPZCGBsjw97+msibjvEa8ZTie5z4CF/Sda1gtGt3NjLwtQXTFEf3Bycqq+cc/NX3Q4xljVVJSqp7S02PfhDCyImnVyj/GatRB1fMf9dWY5z5qEsign2p19SZqOjkW6wMe64OaaDdZSam5p3AgD/cwkj18G92Yi7ciaOi5j2QM9yjoNwO1XWYaecuC8Ic7OkW3DiJ7IYl8sJWUlGKnWCei61ouLN7vZ32HmxT0m4lYXcJExwHravXXdUVGdDLzlVJk+erbooqn16Kk1NxSvJ/p8Hc7kd9x1GesX0G/BUn0XEVk6yPcU4h17XVdQ1F1lSWeD3tNvzqt6xzI6NEaslJSSnSsX0FfGkVDTk7F08uJrihq+4KYJa+iqO/dQJWUkpkSvburgr40isb+CX5tl/mFK5noq7BqOydSV4rct9ou8wvvY20VUevWybvMVymYSS19abIa86fz9TnHEe/vMer6otW1j3Xd9yb6NZ1EV4o3aUxfJCQZv8eAuv8S8nhJ1hCUmT/3ofMeLTclKt6gn6y/SxRpNJMn+z81KS/3j5MnJ76sc/D00/7BnxB8AAAGYklEQVQPUsz84xNPJLau+pg82W8nvN2sLP+HMHXJyoJFi6qW9emn4Y9/rFwX+L+dhNr/VCg8P5xPmrbj9n8L8dQMjZnU0pegSXbvI56rt5zTr8KbejpeY/pq6YukWLJ7H5HrKy31jzX1iGbPhrZt615XWprvZYR7GqC/J20sH3xwfNaroC/ShDRk6Ko+24ocHooO5m3bwpNP+nyRFUl5eWV7NN7KoEcP/a1ponJyjs96FfRFAqyhvYzo5WsK7G3b+l7FI49UP3+RkeErDudg+vRk7VXzZ+bfs+MinjGgxkwa0xdp3mJd3lrXlVeR/wiWluavUIr+gV5dt2SO95LY5nDpbKKIc0zffN6mo6CgwBUVFaW6GCLShBUWwrRpcPhw5by2bSt7J4WFMHOmHxfPyYGLL4bly6tOP/lk1eXT0+GEE6CkxLe0I0NjejocO1Z3mdLSoKwsdtnD2/nkE7+d8vLqeXr08D2oRJjZGudcQax8Gt4RkWYn+tLX6OGo6HMj//mf1aejl1+4EA4cqHmoa+HCusvTo4evRKZPr35uIz3dD3tFb6e8HJ56qvrJ9PBw2PGilr6ISBxyc2HXrurzo1vl0b2M2bPrPjeSaP7axNvSV9AXEYlDrCGlVNPwjohIEsUaUmouWqe6ACIizUX4NwvNmVr6IiIBoqAvIhIgCvoiIgGioC8iEiAK+iIiAdLkrtM3s/1ADT+BiEtX4EASi9McaJ+DQfscDA3Z5x7OuRNjZWpyQb8hzKwonh8ntCTa52DQPgdDY+yzhndERAJEQV9EJEBaWtB/ItUFSAHtczBon4PhuO9zixrTFxGRurW0lr6IiNShxQR9MxtrZlvNbLuZzUh1eZLFzLqb2Uoz22RmG83sttD8Lmb2qpltCz12Ds03M3s09D6sN7P81O5B/ZhZmpn93cxeDE33NLO3Qvv1f80sIzS/TWh6e+j13FSWu77MrJOZPWNmW8xss5mdHYBjfHvoM/2OmS02s8yWeJzNbIGZ7TOzdyLmJXxszWxqKP82M5ta3/K0iKBvZmnAPOAiIA+YZGZ5qS1V0pQC/+qcywPOAm4O7dsM4DXnXG/gtdA0+PegdyhNA+Y3fpGT4jZgc8T0A8DDzrnTgU+B60Pzrwc+Dc1/OJSvOXoEeMU51wcYhN/3FnuMzexU4FagwDnXH0gDJtIyj/PvgLFR8xI6tmbWBbgHGAYMBe4JVxQJi+ePdJt6As4GVkRM3wncmepyHad9/QNwAbAVODk072Rga+j548CkiPwV+ZpLArJDX4TzgBcBw/9gpXX08QZWAGeHnrcO5bNU70OC+9sR2BFd7hZ+jE8FPgS6hI7bi8CFLfU4A7nAO/U9tsAk4PGI+VXyJZJaREufyg9QWHFoXosS6tIOAd4Cvumc+yj00sfAN0PPW8J7MRf4CRD+y+gs4KBzrjQ0HblPFfsbev1QKH9z0hPYDywMDWn9xsza0YKPsXNuNzAH+AD4CH/c1tCyj3OkRI9t0o55Swn6LZ6ZtQeeBf7FOfdZ5GvOV/0t4jIsM/tfwD7n3JpUl6URtQbygfnOuSHAl1R294GWdYwBQkMT4/AV3ilAO6oPgQRCYx/blhL0dwPdI6azQ/NaBDNLxwf8Qufcf4Vm7zWzk0OvnwzsC81v7u/FcOAyM9sJLMEP8TwCdDKz8D+9Re5Txf6GXu8IlDRmgZOgGCh2zr0Vmn4GXwm01GMMcD6wwzm33zl3DPgv/LFvycc5UqLHNmnHvKUE/dVA79CZ/wz8CaFlKS5TUpiZAb8FNjvnHop4aRkQPoM/FT/WH55/degqgLOAQxHdyCbPOXency7bOZeLP46vO+cmAyuBCaFs0fsbfh8mhPI3qxaxc+5j4EMz+3Zo1mhgEy30GId8AJxlZm1Dn/HwPrfY4xwl0WO7AhhjZp1DvaQxoXmJS/UJjiSeKLkYeBd4D5iZ6vIkcb/OwXf91gPrQuli/Hjma8A24I9Al1B+w1/J9B6wAX91RMr3o577fi7wYuh5L+BtYDvw/4A2ofmZoentodd7pbrc9dzXwUBR6Dg/D3Ru6ccYuA/YArwDPA20aYnHGViMP29xDN+ru74+xxa4LrT/24Fr61se/SJXRCRAWsrwjoiIxEFBX0QkQBT0RUQCREFfRCRAFPRFRAJEQV9EJEAU9EVEAkRBX0QkQP4/GM9W3LvA0mwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7acfc87048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_graphics (history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
