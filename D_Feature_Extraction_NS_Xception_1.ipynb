{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Entreno de un ConvNet Xception preentrenado con Imagenet y\n",
    "entrenando conjunto de datos pequeño\n",
    "Conjunto de datos de 4 clases\n",
    "Utilizaremos un conjunto de datos reducido:\n",
    "- Conjunto de entrenamiento: 1.000 muestras x clase \n",
    "- Conjunto de validación: 300 muestras x clase\n",
    "- Conjunto de test: 300 muestras x clase\n",
    "El conjunto de datos: balanced_dataset\n",
    "contiene tres carpetas, train, validation y test, cada una con 4 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image  import ImageDataGenerator, img_to_array, load_img \n",
    "from keras.models  import Sequential\n",
    "from keras.layers  import Dropout,  Flatten,  Dense, GlobalAveragePooling2D, Input\n",
    "from keras.applications import InceptionV3\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical \n",
    "import time\n",
    "import os\n",
    "import numpy as np \n",
    "import math\n",
    "import  matplotlib.pyplot  as  plt\n",
    "\n",
    "img_width = 299\n",
    "img_height = 299\n",
    "batch_size = 64 \n",
    "epochs = 90\n",
    "\n",
    "nb_train = 4000\n",
    "nb_validation = 1200\n",
    "nb_test = 1200\n",
    "nb_classes = 4\n",
    "nb_FC = 1024\n",
    "channels_image = 3\n",
    "\n",
    "class_mode = 'categorical'\n",
    "train_dir  =  \"balanced_dataset/train\"\n",
    "validation_dir = \"balanced_dataset/validation\" \n",
    "test_dir  =  \"balanced_dataset/test\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 6s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import Xception\n",
    "#Load Pretrained model\n",
    "conv_base = Xception(include_top=False, \n",
    "                     weights='imagenet', \n",
    "                     input_shape = (img_width, img_height, channels_image)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 299, 299, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 149, 149, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 149, 149, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 149, 149, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 147, 147, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 147, 147, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 147, 147, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 147, 147, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 147, 147, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 147, 147, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 74, 74, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 74, 74, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 74, 74, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 74, 74, 128)  0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 74, 74, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 74, 74, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 74, 74, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 37, 37, 256)  32768       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 37, 37, 256)  1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 37, 37, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 37, 37, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 37, 37, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 37, 37, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 37, 37, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 19, 19, 728)  186368      add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 19, 19, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 19, 19, 728)  2912        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 19, 19, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 19, 19, 728)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 19, 19, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 19, 19, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 19, 19, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 19, 19, 728)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 19, 19, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 19, 19, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 19, 19, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 19, 19, 728)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 19, 19, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 19, 19, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 19, 19, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 19, 19, 728)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 19, 19, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 19, 19, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 19, 19, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 19, 19, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 19, 19, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 19, 19, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 19, 19, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 19, 19, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 19, 19, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 19, 19, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 19, 19, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 19, 19, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 19, 19, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 19, 19, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 19, 19, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 19, 19, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 19, 19, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 19, 19, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 19, 19, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 10, 10, 1024) 745472      add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 10, 10, 1024) 0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 10, 10, 1024) 4096        conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 10, 10, 1024) 0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 10, 10, 1536) 1582080     add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 10, 10, 1536) 6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 10, 10, 1536) 0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 10, 10, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 10, 10, 2048) 8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 10, 10, 2048) 0           block14_sepconv2_bn[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 20,861,480\n",
      "Trainable params: 20,806,952\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION (Extracción de características)\n",
    "\n",
    "Ahora hay dos formas de proceder:\n",
    "1- Ejecutar la base convolucional sobre el conjunto de datos, registrar la salida en una matriz Numpy en disco, y luego usar esa información como entrada para un clasificador independiente y densamente conectado. Esta solución es rápida y económica computacionalmente, pero no permite utilizar Data Augmentation.\n",
    "La primera parte es la base convolucional de un modelo preentrenado, segundo se entrena la parte convolucional con los nuevos datos de entrenamiento, depués añadimos un nuevo clasificador densamente conectado en la parte superior (final) y volvemos a entrenar. Si el modelo con el que se entrenó el modelo preentrenado es muy difernte del que queremos aplicar, es conveniente no utilizar toda la base convolucional (eliminar las últimas) ya que éstas són cada vez más especificas al conjunto de datos con el que se entrenó.\n",
    "2- Amplicar el modelo que tenemos (conv_base) agregando capas densas en la parte superior y ejecutando todo de punta a punta en los datos de entrada. Esto permite Data Augmentation, porque cada imagen de entrada pasa por la base convolucional. Esta técnica es mucho mas costosa y requiere GPU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. Extracción de características sin Data Augmentation --> CPU\n",
    "\n",
    "Ejecutamos instancias de ImageDataGenerator para extraer imagenes como matrices Numpy y sus etiquetas. Extraeremos características de estas imágenes llamando al método del modelo conv_base\n",
    "\n",
    "- La ultima salida de la capa convolucional es de (8, 8, 2048 )\n",
    "out_x, out_y, conv_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 4 classes.\n",
      "Found 1200 images belonging to 4 classes.\n",
      "Found 1200 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "#Extracción de características del modelo Preentrenado y nuestro dataset\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "#Tamaño de salida de la última capa convoluciónal del modelo preentrenado\n",
    "#lo vemos en el conv_base.summary()\n",
    "out_x = 10\n",
    "out_y = 10\n",
    "conv_len = 2048\n",
    "\n",
    "datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count, out_x, out_x, conv_len))\n",
    "    labels = to_categorical(np.zeros(shape=(sample_count)),nb_classes) #INDICAR NUMERO DE CLASES\n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory,\n",
    "        target_size = (img_height, img_width),\n",
    "        batch_size = batch_size,\n",
    "        #classes = 4,\n",
    "        class_mode = 'categorical')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch) ##Asociamos al modelo preentrenado\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count:\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "#Train: 1000 muestras x clase, con 4 clases, 1000 x 4 = 4000\n",
    "train_features, train_labels = extract_features(train_dir, nb_train)\n",
    "#validation 500 muestras x clase con 4 clases, 300 x 4 = 1200\n",
    "validation_features, validation_labels = extract_features(validation_dir, nb_validation) \n",
    "#test 500 muestras x clase con 4 clases, 300 x 4 = 1200\n",
    "test_features, test_labels = extract_features(test_dir, nb_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('Xception/Xception_FE_NS_train_1.npy',  train_features)\n",
    "np.save('Xception/Xception_FE_NS_validation_1.npy',  validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 10, 10, 2048)\n",
      "(1200, 10, 10, 2048)\n",
      "(1200, 10, 10, 2048)\n"
     ]
    }
   ],
   "source": [
    "print(train_features.shape)\n",
    "print(validation_features.shape)\n",
    "print(test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Save Features\n",
    "train_data = np.load('Xception/Xception_FE_NS_train_1.npy') \n",
    "validation_data  =  np.load('Xception/Xception_FE_NS_validation_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models  import Model\n",
    "# add a global spatial average pooling layer\n",
    "inputTensor = Input((10,10,2048)) #Use (None,None,2048) if bottlenecks vary in size    \n",
    "x = GlobalAveragePooling2D()(inputTensor)\n",
    "\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "\n",
    "# and a logistic layer -- let's say we have 2 classes\n",
    "predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "# What is the correct input? Obviously not base_model.input.\n",
    "model = Model(inputTensor, predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in conv_base.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile models\n",
    "model.compile(\n",
    "    optimizer = RMSprop(lr=2e-5),\n",
    "    loss='categorical_crossentropy',  \n",
    "    metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1200 samples\n",
      "Epoch 1/90\n",
      "4000/4000 [==============================] - 6s 2ms/step - loss: 1.2687 - acc: 0.4662 - val_loss: 1.1648 - val_acc: 0.5592\n",
      "Epoch 2/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 1.0940 - acc: 0.5955 - val_loss: 1.0420 - val_acc: 0.5975\n",
      "Epoch 3/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.9928 - acc: 0.6282 - val_loss: 0.9634 - val_acc: 0.6225\n",
      "Epoch 4/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.9205 - acc: 0.6628 - val_loss: 0.9045 - val_acc: 0.6450\n",
      "Epoch 5/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.8642 - acc: 0.6800 - val_loss: 0.8562 - val_acc: 0.6642\n",
      "Epoch 6/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.8194 - acc: 0.6995 - val_loss: 0.8124 - val_acc: 0.6833\n",
      "Epoch 7/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.7820 - acc: 0.7177 - val_loss: 0.7830 - val_acc: 0.7042\n",
      "Epoch 8/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.7498 - acc: 0.7212 - val_loss: 0.7558 - val_acc: 0.7083\n",
      "Epoch 9/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.7214 - acc: 0.7403 - val_loss: 0.7300 - val_acc: 0.7183\n",
      "Epoch 10/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6960 - acc: 0.7475 - val_loss: 0.7051 - val_acc: 0.7200\n",
      "Epoch 11/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6739 - acc: 0.7578 - val_loss: 0.6816 - val_acc: 0.7383\n",
      "Epoch 12/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6533 - acc: 0.7687 - val_loss: 0.6704 - val_acc: 0.7392\n",
      "Epoch 13/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6343 - acc: 0.7758 - val_loss: 0.6523 - val_acc: 0.7508\n",
      "Epoch 14/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.6171 - acc: 0.7840 - val_loss: 0.6407 - val_acc: 0.7517\n",
      "Epoch 15/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.6014 - acc: 0.7897 - val_loss: 0.6199 - val_acc: 0.7575\n",
      "Epoch 16/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.5879 - acc: 0.7925 - val_loss: 0.6157 - val_acc: 0.7542\n",
      "Epoch 17/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.5748 - acc: 0.8000 - val_loss: 0.6051 - val_acc: 0.7717\n",
      "Epoch 18/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.5627 - acc: 0.8043 - val_loss: 0.5873 - val_acc: 0.7667\n",
      "Epoch 19/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.5517 - acc: 0.8097 - val_loss: 0.5862 - val_acc: 0.7758\n",
      "Epoch 20/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.5412 - acc: 0.8167 - val_loss: 0.5749 - val_acc: 0.7758\n",
      "Epoch 21/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.5312 - acc: 0.8187 - val_loss: 0.5628 - val_acc: 0.7800\n",
      "Epoch 22/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.5224 - acc: 0.8147 - val_loss: 0.5647 - val_acc: 0.7800\n",
      "Epoch 23/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.5123 - acc: 0.8222 - val_loss: 0.5463 - val_acc: 0.7883\n",
      "Epoch 24/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.5050 - acc: 0.8250 - val_loss: 0.5454 - val_acc: 0.7883\n",
      "Epoch 25/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4966 - acc: 0.8233 - val_loss: 0.5395 - val_acc: 0.7908\n",
      "Epoch 26/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.4894 - acc: 0.8327 - val_loss: 0.5346 - val_acc: 0.7950\n",
      "Epoch 27/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4817 - acc: 0.8333 - val_loss: 0.5234 - val_acc: 0.8017\n",
      "Epoch 28/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4748 - acc: 0.8342 - val_loss: 0.5161 - val_acc: 0.8025\n",
      "Epoch 29/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4681 - acc: 0.8383 - val_loss: 0.5292 - val_acc: 0.7925\n",
      "Epoch 30/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4629 - acc: 0.8420 - val_loss: 0.5206 - val_acc: 0.7925\n",
      "Epoch 31/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4564 - acc: 0.8410 - val_loss: 0.5071 - val_acc: 0.8042\n",
      "Epoch 32/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.4505 - acc: 0.8402 - val_loss: 0.5060 - val_acc: 0.8050\n",
      "Epoch 33/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4443 - acc: 0.8488 - val_loss: 0.5010 - val_acc: 0.8075\n",
      "Epoch 34/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4397 - acc: 0.8508 - val_loss: 0.5136 - val_acc: 0.7950\n",
      "Epoch 35/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4347 - acc: 0.8478 - val_loss: 0.4948 - val_acc: 0.8042\n",
      "Epoch 36/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.4292 - acc: 0.8492 - val_loss: 0.4889 - val_acc: 0.8133\n",
      "Epoch 37/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4248 - acc: 0.8545 - val_loss: 0.4833 - val_acc: 0.8092\n",
      "Epoch 38/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4196 - acc: 0.8533 - val_loss: 0.4868 - val_acc: 0.8183\n",
      "Epoch 39/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4160 - acc: 0.8582 - val_loss: 0.4728 - val_acc: 0.8158\n",
      "Epoch 40/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4107 - acc: 0.8600 - val_loss: 0.4815 - val_acc: 0.8208\n",
      "Epoch 41/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.4074 - acc: 0.8642 - val_loss: 0.4769 - val_acc: 0.8233\n",
      "Epoch 42/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.4012 - acc: 0.8668 - val_loss: 0.4726 - val_acc: 0.8167\n",
      "Epoch 43/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3977 - acc: 0.8660 - val_loss: 0.4615 - val_acc: 0.8175\n",
      "Epoch 44/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3932 - acc: 0.8678 - val_loss: 0.4645 - val_acc: 0.8183\n",
      "Epoch 45/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3887 - acc: 0.8678 - val_loss: 0.4614 - val_acc: 0.8183\n",
      "Epoch 46/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3857 - acc: 0.8678 - val_loss: 0.4536 - val_acc: 0.8225\n",
      "Epoch 47/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3815 - acc: 0.8718 - val_loss: 0.4704 - val_acc: 0.8058\n",
      "Epoch 48/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3786 - acc: 0.8718 - val_loss: 0.4501 - val_acc: 0.8225\n",
      "Epoch 49/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3752 - acc: 0.8728 - val_loss: 0.4522 - val_acc: 0.8217\n",
      "Epoch 50/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3715 - acc: 0.8757 - val_loss: 0.4457 - val_acc: 0.8217\n",
      "Epoch 51/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3682 - acc: 0.8760 - val_loss: 0.4490 - val_acc: 0.8217\n",
      "Epoch 52/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3656 - acc: 0.8760 - val_loss: 0.4497 - val_acc: 0.8292\n",
      "Epoch 53/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3613 - acc: 0.8800 - val_loss: 0.4453 - val_acc: 0.8233\n",
      "Epoch 54/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3579 - acc: 0.8800 - val_loss: 0.4428 - val_acc: 0.8275\n",
      "Epoch 55/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3552 - acc: 0.8805 - val_loss: 0.4358 - val_acc: 0.8267\n",
      "Epoch 56/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3517 - acc: 0.8850 - val_loss: 0.4403 - val_acc: 0.8317\n",
      "Epoch 57/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3482 - acc: 0.8817 - val_loss: 0.4412 - val_acc: 0.8217\n",
      "Epoch 58/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3464 - acc: 0.8828 - val_loss: 0.4324 - val_acc: 0.8317\n",
      "Epoch 59/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3429 - acc: 0.8880 - val_loss: 0.4318 - val_acc: 0.8300\n",
      "Epoch 60/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3399 - acc: 0.8840 - val_loss: 0.4313 - val_acc: 0.8308\n",
      "Epoch 61/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3372 - acc: 0.8892 - val_loss: 0.4277 - val_acc: 0.8317\n",
      "Epoch 62/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3338 - acc: 0.8895 - val_loss: 0.4334 - val_acc: 0.8400\n",
      "Epoch 63/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3312 - acc: 0.8875 - val_loss: 0.4266 - val_acc: 0.8283\n",
      "Epoch 64/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3291 - acc: 0.8918 - val_loss: 0.4228 - val_acc: 0.8325\n",
      "Epoch 65/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3259 - acc: 0.8940 - val_loss: 0.4263 - val_acc: 0.8408\n",
      "Epoch 66/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3242 - acc: 0.8918 - val_loss: 0.4199 - val_acc: 0.8325\n",
      "Epoch 67/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3211 - acc: 0.8925 - val_loss: 0.4177 - val_acc: 0.8333\n",
      "Epoch 68/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3176 - acc: 0.8942 - val_loss: 0.4170 - val_acc: 0.8375\n",
      "Epoch 69/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.3164 - acc: 0.8955 - val_loss: 0.4161 - val_acc: 0.8358\n",
      "Epoch 70/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3138 - acc: 0.8958 - val_loss: 0.4160 - val_acc: 0.8333\n",
      "Epoch 71/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3122 - acc: 0.8955 - val_loss: 0.4189 - val_acc: 0.8450\n",
      "Epoch 72/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3090 - acc: 0.9008 - val_loss: 0.4151 - val_acc: 0.8317\n",
      "Epoch 73/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3073 - acc: 0.8975 - val_loss: 0.4211 - val_acc: 0.8433\n",
      "Epoch 74/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3044 - acc: 0.8997 - val_loss: 0.4149 - val_acc: 0.8317\n",
      "Epoch 75/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.3022 - acc: 0.9010 - val_loss: 0.4190 - val_acc: 0.8433\n",
      "Epoch 76/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2995 - acc: 0.9010 - val_loss: 0.4071 - val_acc: 0.8383\n",
      "Epoch 77/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2980 - acc: 0.9027 - val_loss: 0.4124 - val_acc: 0.8475\n",
      "Epoch 78/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2952 - acc: 0.9027 - val_loss: 0.4089 - val_acc: 0.8392\n",
      "Epoch 79/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2926 - acc: 0.9055 - val_loss: 0.4121 - val_acc: 0.8342\n",
      "Epoch 80/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2912 - acc: 0.9065 - val_loss: 0.4063 - val_acc: 0.8392\n",
      "Epoch 81/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2891 - acc: 0.9035 - val_loss: 0.4100 - val_acc: 0.8433\n",
      "Epoch 82/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2863 - acc: 0.9098 - val_loss: 0.4073 - val_acc: 0.8425\n",
      "Epoch 83/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2846 - acc: 0.9055 - val_loss: 0.4076 - val_acc: 0.8400\n",
      "Epoch 84/90\n",
      "4000/4000 [==============================] - 4s 1ms/step - loss: 0.2833 - acc: 0.9080 - val_loss: 0.4059 - val_acc: 0.8400\n",
      "Epoch 85/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2796 - acc: 0.9105 - val_loss: 0.4002 - val_acc: 0.8425\n",
      "Epoch 86/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2777 - acc: 0.9115 - val_loss: 0.3993 - val_acc: 0.8458\n",
      "Epoch 87/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2755 - acc: 0.9115 - val_loss: 0.4106 - val_acc: 0.8467\n",
      "Epoch 88/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2743 - acc: 0.9140 - val_loss: 0.4033 - val_acc: 0.8417\n",
      "Epoch 89/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2711 - acc: 0.9120 - val_loss: 0.3980 - val_acc: 0.8450\n",
      "Epoch 90/90\n",
      "4000/4000 [==============================] - 5s 1ms/step - loss: 0.2706 - acc: 0.9117 - val_loss: 0.4149 - val_acc: 0.8433\n"
     ]
    }
   ],
   "source": [
    "#Train model\n",
    "\n",
    "history = model.fit(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    epochs=epochs,  \n",
    "    batch_size=batch_size, \n",
    "    validation_data=(validation_data, validation_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Guardamos el modelo\n",
    "# h5py \n",
    "model.save_weights('Xception/Xception_FE_NS_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_graphics (history):\n",
    "    #Mostramos otro tipo de grafico\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    epochs = range(1, len(acc)+ 1)\n",
    "\n",
    "    plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "    #plt.tittle('Trainning and validation accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    #plt.tittle('Trainning and validation loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_evaluate_model(model, history):\n",
    "    #Evaluate model\n",
    "    (loss, acc) = model.evaluate(\n",
    "        test_features, test_labels, \n",
    "        batch_size=batch_size, \n",
    "        verbose=0)\n",
    "\n",
    "    print(\"acc: {0:.2f}% - loss: {1:f}\".format(acc * 100, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb1fa77780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb23e04630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_graphics (history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 82.50% - loss: 0.462230\n"
     ]
    }
   ],
   "source": [
    "print_evaluate_model(model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
